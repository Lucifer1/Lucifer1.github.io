{"meta":{"title":"Lightman's blog","subtitle":"","description":"","author":"Lightman","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Typescript 学习笔记","slug":"Typescript","date":"2020-12-01T02:12:21.000Z","updated":"2020-12-07T15:32:18.852Z","comments":true,"path":"2020/12/01/Typescript/","link":"","permalink":"http://yoursite.com/2020/12/01/Typescript/","excerpt":"","text":"基础原始数据类型1. 布尔值 Typescript中使用boolean来定义布尔值，而不是Boolean，其他的基础类型也是一样 例如： 123let isDone: boolean &#x3D; false; &#x2F;&#x2F; 编译通过let createdByNewBoolean: boolean &#x3D; new Boolean(1); &#x2F;&#x2F; 不通过 因为使用构造函数Boolean创造的对象不是布尔值，new Boolean()事实上返回的是一个Boolean对象，如果改成： 1let createdByNewBoolean: Boolean &#x3D; new Boolean(1); &#x2F;&#x2F; 通过 此外，直接调用Boolean()也可以返回一个boolean类型： 1let createdByBoolean: boolean &#x3D; Boolean(1); 2. 空值 JavaScript 没有空值（Void）的概念，在 TypeScript 中，可以用 void 表示没有任何返回值的函数： 123function alertName(): void &#123; alert(&#39;My name is Tom&#39;);&#125; 声明一个 void 类型的变量没有什么用，因为你只能将它赋值为 undefined 和 null 3. Null Undefined 在 TypeScript 中，可以使用 null 和 undefined 来定义这两个原始数据类型： 12let u: undefined &#x3D; undefined;let n: null &#x3D; null; 与 void 的区别是，undefined 和 null 是所有类型的子类型。也就是说 undefined 类型的变量，可以赋值给 number 类型的变量： 12345&#x2F;&#x2F; 这样不会报错let num: number &#x3D; undefined;&#x2F;&#x2F; 这样也不会报错let u: undefined;let num: number &#x3D; u; 而 void 类型的变量不能赋值给 number 类型的变量： 1234let u: void;let num: number &#x3D; u;&#x2F;&#x2F; Type &#39;void&#39; is not assignable to type &#39;number&#39;. 任意值(Any)1. 什么是任意值类型 任意值：一个普通类型在定义完之后，是不允许被赋值为其他类型的： 1234let myFavoriteNumber: string &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7;&#x2F;&#x2F; index.ts(2,1): error TS2322: Type &#39;number&#39; is not assignable to type &#39;string&#39;. 只有当它为任意值时，才可以赋值为其他类型： 12let myFavoriteNumber: any &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7; 2. 任意值的属性和方法 任意值可以访问任何属性，也可以调用任何方法 3. 未声明类型的变量 在ts中，如果没有给变量定义类型，那么它默认为任意值类型 12345let something;something &#x3D; &#39;seven&#39;;something &#x3D; 7;something.setName(&#39;Tom&#39;); 类型推论 如果没有明确指出，那么TS会依照类型推论原则，推断出一个类型： 1234let myFavoriteNumber &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7;&#x2F;&#x2F; index.ts(2,1): error TS2322: Type &#39;number&#39; is not assignable to type &#39;string&#39;. 根据类型推论，它等价于： 1234let myFavoriteNumber: string &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7;&#x2F;&#x2F; index.ts(2,1): error TS2322: Type &#39;number&#39; is not assignable to type &#39;string&#39;. 如果定义时没有赋值，不管之后有没有赋值，都会被推断成any类型，不会被类型检查： 123let myFavoriteNumber;myFavoriteNumber &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7; 联合类型1. 定义 联合类型（Union Types）表示取值可以为多种类型中的一种: 123let myFavoriteNumber: string | number;myFavoriteNumber &#x3D; &#39;seven&#39;;myFavoriteNumber &#x3D; 7; 12345let myFavoriteNumber: string | number;myFavoriteNumber &#x3D; true;&#x2F;&#x2F; index.ts(2,1): error TS2322: Type &#39;boolean&#39; is not assignable to type &#39;string | number&#39;.&#x2F;&#x2F; Type &#39;boolean&#39; is not assignable to type &#39;number&#39;. 2. 访问联合类型的属性或方法 在使用联合类型时，因为typescript不确定使用的是哪种类型，所以在调用变量的方式时，只能调用所有联合类型共有的方法： 123456function getLength(something: string | number): number &#123; return something.length;&#125;&#x2F;&#x2F; index.ts(2,22): error TS2339: Property &#39;length&#39; does not exist on type &#39;string | number&#39;.&#x2F;&#x2F; Property &#39;length&#39; does not exist on type &#39;number&#39;. 3. 联合属性的赋值 联合类型在被赋值时，会根据类型推论的规则推断出一个类型： 1234567let myFavoriteNumber: string | number;myFavoriteNumber &#x3D; &#39;seven&#39;;console.log(myFavoriteNumber.length); &#x2F;&#x2F; 5myFavoriteNumber &#x3D; 7;console.log(myFavoriteNumber.length); &#x2F;&#x2F; 编译时报错&#x2F;&#x2F; index.ts(5,30): error TS2339: Property &#39;length&#39; does not exist on type &#39;number&#39;. 上例中，第二行的 myFavoriteNumber 被推断成了 string，访问它的 length 属性不会报错。 而第四行的 myFavoriteNumber 被推断成了 number，访问它的 length 属性时就报错了。 接口1. 什么是接口 定义：TypeScript 中的接口是一个非常灵活的概念，除了可用于对类的一部分行为进行抽象以外，也常用于对「对象的形状（Shape）」进行描述。 简单实例 123456789interface Person &#123; name: string; age: number;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;, age: 25&#125;; 注意：变量类型为接口的变量的形状必须与接口一致，多属性或者少属性都是不允许的，都会报错 2. 可选属性 可选属性：希望不用完全匹配时，使用可选属性（但是只能使用定义好的可选属性，还是不能随意添加） 12345678interface Person &#123; name: string; age?: number;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;&#125;; 123456789interface Person &#123; name: string; age?: number;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;, age: 25&#125;; 3. 任意属性 任意属性：允许接口有任意属性时，使用该属性 注意：使用任意属性时，确定属性和可选属性的类型都必须是任意属性的子类型 一个接口中只能拥有一个任意属性，需要使用多个类型是，可以使用联合类型 12345678910interface Person &#123; name: string; age?: number; [propName: string]: any;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;, gender: &#39;male&#39;&#125;; 错误例子： 1234567891011121314151617interface Person &#123; name: string; age?: number; [propName: string]: string;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;, age: 25, gender: &#39;male&#39;&#125;;&#x2F;&#x2F; index.ts(3,5): error TS2411: Property &#39;age&#39; of type &#39;number&#39; is not assignable to string index type &#39;string&#39;.&#x2F;&#x2F; index.ts(7,5): error TS2322: Type &#39;&#123; [x: string]: string | number; name: string; age: number; gender: string; &#125;&#39; is not assignable to type &#39;Person&#39;.&#x2F;&#x2F; Index signatures are incompatible.&#x2F;&#x2F; Type &#39;string | number&#39; is not assignable to type &#39;string&#39;.&#x2F;&#x2F; Type &#39;number&#39; is not assignable to type &#39;string&#39;. 4. 只读属性 只读属性：只能用来读取，不能被赋值，使用readonly来定义 1234567891011121314interface Person &#123; readonly id: number; name: string; age?: number; [propName: string]: any;&#125;let tom: Person &#x3D; &#123; id: 89757, name: &#39;Tom&#39;, gender: &#39;male&#39;&#125;;tom.id &#x3D; 9527; 注意：注意，只读的约束存在于第一次给对象赋值的时候，而不是第一次给只读属性赋值的时候： 1234567891011121314151617interface Person &#123; readonly id: number; name: string; age?: number; [propName: string]: any;&#125;let tom: Person &#x3D; &#123; name: &#39;Tom&#39;, gender: &#39;male&#39;&#125;;tom.id &#x3D; 89757;&#x2F;&#x2F; index.ts(8,5): error TS2322: Type &#39;&#123; name: string; gender: string; &#125;&#39; is not assignable to type &#39;Person&#39;.&#x2F;&#x2F; Property &#39;id&#39; is missing in type &#39;&#123; name: string; gender: string; &#125;&#39;.&#x2F;&#x2F; index.ts(13,5): error TS2540: Cannot assign to &#39;id&#39; because it is a constant or a read-only property. 上例中，报错信息有两处，第一处是在对 tom 进行赋值的时候，没有给 id 赋值。 第二处是在给 tom.id 赋值的时候，由于它是只读属性，所以报错了。 数组的类型1. 「类型 + 方括号」表示法 基本定义： 类型+方括号 1let fibonacci: number[] &#x3D; [1, 1, 2, 3, 5]; 不允许出现其他类型的变量 123let fibonacci: number[] &#x3D; [1, &#39;1&#39;, 2, 3, 5];&#x2F;&#x2F; Type &#39;string&#39; is not assignable to type &#39;number&#39;. 定义好类型后，对应的数组方法的类型也会被限制 1234let fibonacci: number[] &#x3D; [1, 1, 2, 3, 5];fibonacci.push(&#39;8&#39;);&#x2F;&#x2F; Argument of type &#39;&quot;8&quot;&#39; is not assignable to parameter of type &#39;number&#39;. 2. 数组泛型 数组泛型： 1let fibonacci: Array&lt;number&gt; &#x3D; [1, 1, 2, 3, 5]; 3. 用接口表示数组 用接口表示数组：这种方法不常用 1234interface NumberArray &#123; [index: number]: number;&#125;let fibonacci: NumberArray &#x3D; [1, 1, 2, 3, 5]; NumberArray 表示：只要索引的类型是数字时，那么值的类型必须是数字。 不过它经常用来表示类数组 4. 类数组 类数组：比如说arguments 12345function sum() &#123; let args: number[] &#x3D; arguments;&#125;&#x2F;&#x2F; Type &#39;IArguments&#39; is missing the following properties from type &#39;number[]&#39;: pop, push, concat, join, and 24 more. 它不能用普通的数组来表示，一般使用接口的方式来表示 1234567function sum() &#123; let args: &#123; [index: number]: number; length: number; callee: Function; &#125; &#x3D; arguments;&#125; 除了需要定义每个索引为数字时，变量也必须为数字外，还需要定义length和callee 常用的类数组都有自己的接口定义，类似于:IArguments,NodeList,HTMLCollection等 123function sum() &#123; let args: IArguments &#x3D; arguments;&#125; 其中 IArguments 是 TypeScript 中定义好了的类型，它实际上就是： 12345interface IArguments &#123; [index: number]: any; length: number; callee: Function;&#125; 5. Any在数组中的应用 any在数组中的应用：表示数组中的变量可以为任意类型 1let list: any[] &#x3D; [&#39;xcatliu&#39;, 25, &#123; website: &#39;http:&#x2F;&#x2F;xcatliu.com&#39; &#125;]; 函数的类型1. 函数声明与函数表达式 函数声明在JS中有两种方式：函数声明和函数表达式。在TS中，需要对函数的输入和输出都进行约束 函数声明在TS中很简单： 123function sum(x: number, y: number): number &#123; return x + y;&#125; 这时候，不允许输入多余的参数，或者输入的参数少于形参的数量 123456function sum(x: number, y: number): number &#123; return x + y;&#125;sum(1, 2, 3);&#x2F;&#x2F; index.ts(4,1): error TS2346: Supplied parameters do not match any signature of call target. 123456function sum(x: number, y: number): number &#123; return x + y;&#125;sum(1);&#x2F;&#x2F; index.ts(4,1): error TS2346: Supplied parameters do not match any signature of call target. 函数表达式 123let mySum &#x3D; function (x: number, y: number): number &#123; return x + y;&#125;; 简单来写是这样的，不过这种形式没有明面上对mySum进行约束，而是通过类型推论来推断来的，完整的写法应该是： 123let mySum: (x: number, y: number) &#x3D;&gt; number &#x3D; function (x: number, y: number): number &#123; return x + y;&#125;; 这里的=&gt;和ES6中的箭头函数不同，它只是用来说明函数的输出类型的 2. 用接口定义函数的形状 用接口的形状来定义函数： 12345678interface SearchFunc &#123; (source: string, subString: string): boolean;&#125;let mySearch: SearchFunc;mySearch &#x3D; function(source: string, subString: string) &#123; return source.search(subString) !&#x3D;&#x3D; -1;&#125; 采用函数表达式|接口定义函数的方式时，对等号左侧进行类型限制，可以保证以后对函数名赋值时保证参数个数、参数类型、返回值类型不变。 3. 可选参数 可选参数：它的用法和接口中很类似，都是使用?来定义 123456789function buildName(firstName: string, lastName?: string) &#123; if (lastName) &#123; return firstName + &#39; &#39; + lastName; &#125; else &#123; return firstName; &#125;&#125;let tomcat &#x3D; buildName(&#39;Tom&#39;, &#39;Cat&#39;);let tom &#x3D; buildName(&#39;Tom&#39;); 注意，可选参数后边不能再接必需参数 1234567891011function buildName(firstName?: string, lastName: string) &#123; if (firstName) &#123; return firstName + &#39; &#39; + lastName; &#125; else &#123; return lastName; &#125;&#125;let tomcat &#x3D; buildName(&#39;Tom&#39;, &#39;Cat&#39;);let tom &#x3D; buildName(undefined, &#39;Tom&#39;);&#x2F;&#x2F; index.ts(1,40): error TS1016: A required parameter cannot follow an optional parameter. 4. 参数默认值 参数默认值：ES6中允许给函数添加默认值，TS中会把添加了默认值的参数识别成可选参数 12345function buildName(firstName: string, lastName: string &#x3D; &#39;Cat&#39;) &#123; return firstName + &#39; &#39; + lastName;&#125;let tomcat &#x3D; buildName(&#39;Tom&#39;, &#39;Cat&#39;);let tom &#x3D; buildName(&#39;Tom&#39;); 此时就不再受可选参数必需在必需参数之后的约束了 12345function buildName(firstName: string &#x3D; &#39;Tom&#39;, lastName: string) &#123; return firstName + &#39; &#39; + lastName;&#125;let tomcat &#x3D; buildName(&#39;Tom&#39;, &#39;Cat&#39;);let cat &#x3D; buildName(undefined, &#39;Cat&#39;); 5. 剩余参数 剩余参数：ES6 中，可以使用 …rest 的方式获取函数中的剩余参数（rest 参数），事实上，…rest是一个数组。所以我们可以用数组的类型来定义它： 12345678function push(array: any[], ...items: any[]) &#123; items.forEach(function(item) &#123; array.push(item); &#125;);&#125;let a &#x3D; [];push(a, 1, 2, 3); 6. 重载 重载：重载允许一个函数接受不同数量或类型的参数时，作出不同的处理。 比如，我们需要实现一个函数 reverse，输入数字 123 的时候，输出反转的数字 321，输入字符串 ‘hello’ 的时候，输出反转的字符串 ‘olleh’。 利用联合类型，我们可以这么实现： 1234567function reverse(x: number | string): number | string &#123; if (typeof x &#x3D;&#x3D;&#x3D; &#39;number&#39;) &#123; return Number(x.toString().split(&#39;&#39;).reverse().join(&#39;&#39;)); &#125; else if (typeof x &#x3D;&#x3D;&#x3D; &#39;string&#39;) &#123; return x.split(&#39;&#39;).reverse().join(&#39;&#39;); &#125;&#125; 然而这样有一个缺点，就是不能够精确的表达，输入为数字的时候，输出也应该为数字，输入为字符串的时候，输出也应该为字符串。 这时，我们可以使用重载定义多个 reverse 的函数类型： 123456789function reverse(x: number): number;function reverse(x: string): string;function reverse(x: number | string): number | string &#123; if (typeof x &#x3D;&#x3D;&#x3D; &#39;number&#39;) &#123; return Number(x.toString().split(&#39;&#39;).reverse().join(&#39;&#39;)); &#125; else if (typeof x &#x3D;&#x3D;&#x3D; &#39;string&#39;) &#123; return x.split(&#39;&#39;).reverse().join(&#39;&#39;); &#125;&#125; 上例中，我们重复定义了多次函数 reverse，前几次都是函数定义，最后一次是函数实现。在编辑器的代码提示中，可以正确的看到前两个提示。 注意，TypeScript 会优先从最前面的函数定义开始匹配，所以多个函数定义如果有包含关系，需要优先把精确的定义写在前面。 类型断言类型断言（Type Assertion）可以用来手动指定一个值的类型。 1. 语法 语法： 1值 as 类型 或 1&lt;类型&gt;值 在 tsx 语法（React 的 jsx 语法的 ts 版）中必须使用前者，即 值 as 类型。 形如 的语法在 tsx 中表示的是一个 ReactNode，在 ts 中除了表示类型断言之外，也可能是表示一个泛型。 故建议大家在使用类型断言时，统一使用 值 as 类型 这样的语法 2. 断言的用途 断言的用途： 将一个联合类型断言为其中一个类型 因为ts在联合类型时，没有确定是哪个类型时，只能使用联合类型共有的方法，但是有时我们需要提前使用，这时候就可以使用断言 123456function isFish(animal: Cat | Fish) &#123; if (typeof (animal as Fish).swim &#x3D;&#x3D;&#x3D; &#39;function&#39;) &#123; return true; &#125; return false;&#125; 将一个父类断言为更加具体的子类 12345678910111213class ApiError extends Error &#123; code: number &#x3D; 0;&#125;class HttpError extends Error &#123; statusCode: number &#x3D; 200;&#125;function isApiError(error: Error) &#123; if (typeof (error as ApiError).code &#x3D;&#x3D;&#x3D; &#39;number&#39;) &#123; return true; &#125; return false;&#125; 将任何一个类型断言为any 理想情况下，TypeScript 的类型系统运转良好，每个值的类型都具体而精确。 当我们引用一个在此类型上不存在的属性或方法时，就会报错： 1234const foo: number &#x3D; 1;foo.length &#x3D; 1;&#x2F;&#x2F; index.ts:2:5 - error TS2339: Property &#39;length&#39; does not exist on type &#39;number&#39;. 上面的例子中，数字类型的变量 foo 上是没有 length 属性的，故 TypeScript 给出了相应的错误提示。 这种错误提示显然是非常有用的。 但有的时候，我们非常确定这段代码不会出错，比如下面这个例子： 1234window.foo &#x3D; 1;&#x2F;&#x2F; index.ts:1:8 - error TS2339: Property &#39;foo&#39; does not exist on type &#39;Window &amp; typeof globalThis&#39;. 上面的例子中，我们需要将 window 上添加一个属性 foo，但 TypeScript 编译时会报错，提示我们 window 上不存在 foo 属性。 此时我们可以使用 as any 临时将 window 断言为 any 类型： 1(window as any).foo &#x3D; 1; 在 any 类型的变量上，访问任何属性都是允许的。 需要注意的是，将一个变量断言为 any 可以说是解决 TypeScript 中类型问题的最后一个手段。 它极有可能掩盖了真正的类型错误，所以如果不是非常确定，就不要使用 as any。 上面的例子中，我们也可以通过[扩展 window 的类型（TODO）][]解决这个错误，不过如果只是临时的增加 foo 属性，as any 会更加方便。 总之，一方面不能滥用 as any，另一方面也不要完全否定它的作用，我们需要在类型的严格性和开发的便利性之间掌握平衡（这也是 TypeScript 的设计理念之一），才能发挥出 TypeScript 最大的价值。 将any断言为一个具体的类型 比如遇见历史遗留问题时，别人的写的代码全是any，我们为了明确类型，可以对变量或者返回值进行断言然后返回，这样提高代码的可维护性 1234567891011function getCacheData(key: string): any &#123; return (window as any).cache[key];&#125;interface Cat &#123; name: string; run(): void;&#125;const tom &#x3D; getCacheData(&#39;tom&#39;) as Cat;tom.run(); 3. 类型断言的限制 明确一点：并不是任何一个类型都可以被断言为任何另一个类型，具体来说，若 A 兼容 B，那么 A 能够被断言为 B，B 也能被断言为 A。 4. 双重断言 因为任何类型都可以被断言为any，any 可以被断言为任何类型，所以使用双重断言as any as Foo就可以将任何一个类型转换为任何另一个类型 若你使用了这种双重断言，那么十有八九是非常错误的，它很可能会导致运行时错误。除非迫不得已，千万别用双重断言。** 5. 类型断言 vs 类型转换 类型断言只会影响 TypeScript 编译时的类型，类型断言语句在编译结果中会被删除： 123456function toBoolean(something: any): boolean &#123; return something as boolean;&#125;toBoolean(1); // 返回值为 1 在上面的例子中，将 something 断言为 boolean 虽然可以通过编译，但是并没有什么用，代码在编译后会变成： 123456function toBoolean(something) &#123; return something;&#125;toBoolean(1);&#x2F;&#x2F; 返回值为 1 所以类型断言不是类型转换，它不会真的影响到变量的类型。 若要进行类型转换，需要直接调用类型转换的方法： 123456function toBoolean(something: any): boolean &#123; return Boolean(something);&#125;toBoolean(1);&#x2F;&#x2F; 返回值为 true 6. 类型断言 vs 类型声明 他们两个的区别主要体现在兼容性上 animal 断言为 Cat，只需要满足 Animal 兼容 Cat 或 Cat 兼容 Animal 即可 animal 赋值给 tom，需要满足 Cat 兼容 Animal 才行 类型声明是比类型断言更加严格的。所以为了增加代码的质量，我们最好优先使用类型声明，这也比类型断言的 as 语法更加优雅。 7. 类型断言 vs 泛型声明文件当使用第三方库时，我们需要引用它的声明文件，才能获得对应的代码补全、接口提示等功能。 1. 什么是声明语句2. 什么是声明文件 又一个小标签","categories":[{"name":"前端技术","slug":"前端技术","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Typescript","slug":"Typescript","permalink":"http://yoursite.com/tags/Typescript/"}]},{"title":"Vue","slug":"Vue","date":"2020-06-17T11:03:09.673Z","updated":"2020-01-30T14:48:28.723Z","comments":true,"path":"2020/06/17/Vue/","link":"","permalink":"http://yoursite.com/2020/06/17/Vue/","excerpt":"","text":"[TOC] Vue注意 DOM里边使用定义的别名，需要在前边加上“~” this.$router 拿到的是vue-router这个对象，this.$route 拿到的是当前哪个路由处于活跃状态 ES6语法 ES6除了’’和””以外，还可以使用``（TAB上边的那个点）来定义字符串，他的好处是可以直接回车换行 1. 计算属性 计算属性(computed)本质上是一个属性，它有getter和setter，但是我们在使用时只是实现了getter，而没有实现setter 计算属性在使用多次时，只会调用1次（因为他是有缓存的，效率比methods要高） v-show与v-if的区别：v-show是修改diplay属性，而v-if是dom是否渲染，是否存在 2. JavaScript高阶函数 回调函数： 被用于高级函数里边，作为参数传给高级函数的函数就称为回调函数。 还有一种说法是，用来规范js函数的调用顺序。 obj.filter(callbackfn) 参数为回调函数，对obj执行callbackfn进行过滤 obj.map() 对obj进行操作 obj.reduce() 对obj进行汇总 3. v-model label要想互斥（只能2选1），要给label里边的input的name属性赋相同的值 在vue里边，如果给label里边的input的value属性绑定相同的data也可以互斥 v-model本质为，v-bind:value + v-on:input=”data = $event.target.value” v-model绑定checkbox，绑定单个值，data为boolean类型。绑定多个值时，只需要给data设置为[](list类型)即可 修饰符 lazy：v-model是实时绑定的，当使用v-model.lazy时，可以实现用户敲回车或者点击空白（失去焦点）的时候再进行刷新绑定 number：v-model进行值绑定时，默认为string类型，这时候使用v-model.number trim：v-model去掉多余的空格 4. 组件化 什么是组件化？ 将大的问题拆成一个又一个小的问题去解决 Vue的组件化思想 可以让我们用一个又一个独立可复用的小组件来完成我们的应用 任何应用都可以抽象成一个组件树（Vue官方有图） 全局组件与局部组件 Vue.extend()来编写组件的构造器，但是现在用的很少了，一般直接使用语法糖 const myComponent = Vue.extend({ template: ` &lt;div&gt;lalalalal&lt;/div&gt; ` }) Vue.component(&apos;cp1&apos;, myComponent) #这里注册 通过Vue.component(‘标签名’,组件)来注册的组件为全局组件 在Vue示例当中，也就是new Vue({components:})里边注册的组件为局部组件 组件注册的语法糖 就是省略Vue.extend() Vue.component(&apos;cp1&apos;, { template: ` &lt;div&gt;lalalalal&lt;/div&gt; ` }) v-bind不能识别驼峰，要将驼峰进行转换，mustache是可以识别驼峰的 #组件之间传值 父传子：在子组件的props中定义数据（也可以称之为变量），然后在使用的时候绑定即可 例如：你在子组件中定义了name props: { name: String //这里的String是限制传过来的数据类型，可以不写 } 或者 props: { name: { type: String, default: 这是默认值 } } 然后在使用的地方加上name属性即可，例如cpn组件 &lt;cpn name=&apos;我是一个字符串&apos;&gt;&lt;/cpn&gt; &lt;cpn :name=&apos;父组件中的属性&apos;&gt;&lt;/cpn&gt; //使用v-bind来绑定父组件中的属性 子传父：例如子组件为。 使用自定义事件，this.$emit(‘传递的名称’, 传递的参数)，然后在组件进行监听&lt;cpn @传递的名称=’父组件的函数’&gt; this.$emit(&apos;btnClick&apos;, item) &lt;cpn @btnClick=&apos;cpnClick(item)&apos;&gt; #(item)可以省略，系统默认把子组件传递的参数传给父组件 #其实这里也是不能用驼峰的，但是脚手架里貌似可以用例如，点击button时向父组件传递数据 &lt;template&gt; &lt;div&gt; &lt;button @click=&quot;btnClick&quot;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/template&gt; methods: { btnClick() { this.$emit(&apos;随便起的名称&apos;, 这是数据) } } &lt;cpn @随便起的名称=&quot;父组件的函数(这里是数据)&quot;&gt;&lt;/cpn&gt; 5. 父子组件间的访问方式有时候父组件需要直接访问子组件，而不是使用props通信的方式，子组件需要直接访问父组件或者根组件 父组件访问子组件 $children 在父组件的methods里边任意的函数里边this.$children即可获取子组件，返回的是一个数组（在开发时一般不使用这个方法，只有我们需要拿所有子组件时才可能会用，一般使用$refs） $refs 这个需要在子组件加一个ref属性，然后再在methods里边调用，默认时为空[] &lt;cpn ref=&apos;aaa&apos;&gt;&lt;/cpn&gt; this.$refs.aaa 子组件访问父组件 $parent 用的更少，因为不好复用，不建议使用，在子组件里this.$parent, $root 访问根组件 6. 插槽 slot基本使用 如果在插槽中放东西，就是设置默认值，例如: &lt;slot&gt;&lt;button&gt;&lt;/button&gt;&lt;/slot&gt;如果在使用组件时，没有使用插槽，就会显示默认值，使用了，就显示你想要的值 具名插槽slot ，在使用时要给使用的地方加slot属性，例如 作用域插槽slot 父组件替换插槽里边的标签，但是内容是由子组件来提供， 例如cpn组件里边有一个languages的list &lt;cpn&gt; &lt;template slot=&apos;具名插槽&apos; slot-scope=&apos;myData&apos;&gt; #这里的slot是可选选项，也就是说，作用域插槽可以用到具名插槽里边，myData也相当于是一个变量名称，可以随意定 &lt;span&gt;{{myData.languages}}&lt;/span&gt; #在这里使用这种形来调用子组件中的数据 &lt;/template&gt; &lt;/cpn&gt; &lt;template id=&apos;cpn&apos;&gt; &lt;slot :data=&apos;languages&apos;&gt;&lt;/slot&gt; #这里的data可以为任意名称，这里相当于声明一个变量 &lt;/template&gt; cpn: { template: &quot;#cpn&quot; data: { return { languages : [&apos;java&apos;,&apos;java&apos;,&apos;java&apos;] } } } 7. 模块化开发 ES5模块化 ES5没有模块化，所以使用匿名函数的解决方案 常见的模块化规范 CommonJS、AMD、CMD、ES6的Modules CommenJS(Webpack和Node.js用的多)： 导出： module.exports = { flat: true, test(a, b){ return a + b } } 导入： let obj = require(&apos;xxx.js&apos;) let {flat, tent} = require(&apos;xxx.js&apos;) 也可以直接获取 ES6的模块化 模块化就是在script标签里边的type属性定义为module，然后在该js文件里边export导出，在使用的地方import导入即可，例如: import {name} from ‘xxx.js’ export default： 某些情况下，某个模块包含某个功能，我们并不希望给这个功能命名，而且让导入者自己命名，这时候我们就可以使用export default，一个js文件中只能有一个export default不允许存在多个，并且它后边只能跟一个值或对象，这里再使用import时可以不用使用{}了，直接为： import 我自己的命名 from ‘xxx.js’ 当我们需要导入的东西太多时，例如: import {name,age,aaa,bbb,ccc,ddd} from &apos;aaa.js&apos; 我们可以使用 import * as myName from &apos;aaa/js&apos; myName.name 8. Webpack 基本使用 项目中要使用node依赖包时，要使用npm init命令安装依赖，这时候系统会生成package.json webpack进行打包时，要有一个入口文件，例如main.js，在这个里边对你使用的各个文件，各个模块进行使用或者导入，然是在使用webpack对入口文件进行打包，webpack会自动处理与入口文件有关的各个模块各个依赖，然后进行打包，打包完成之后，webpack会生成一个js，在你想使用的页面中，使用打包后的js即可 require(&apos;./css/normal.css&apos;) require(&apos;./css/special.less&apos;) const {param1, param2} = require(./js/abc.js) param1和param2为接收的变量 - webpack.config.js是webpack的配置文件，在里边对其进行配置，包括entry，output，loader等 const path = require(&apos;path&apos;) module.exports = { entry: &apos;.src/main.js&apos;, output: { // path: 这里必须是绝对地址，path是node的一个库，__dirname是获取本文件的绝对路径，path.resolve是拼接路径 path: path.resolve(__dirname, &apos;dist&apos;) filename: &apos;bundle.js&apos; } } - package.json里的scripts里边定义的指令，优先使用本地库，如果没有本地库才去找全局库，在所有cmd中，都优先使用全局库，所以当我们想使用本地库时，在package.json里边定义指令即可 - npm install webpack --save-dev (--save-dev 是开始时依赖的意思，打包之后不用) 核心为loader，可以在官方文档里边查找使用方法，用来处理各个文件，进行打包 webpack 使用多个loader时，它是从右向左读取的 url-loader 在使用时，options里边有一个limit选项，这里边限制了图片的大小，如果小于这个限制，url-loader会把图片编译成base64格式进行使用，大于这个限制还需要下载一个file-loader ES6 To ES5 babel-loader 脚手架里会有详细配置 vue npm install vue-loader vue-template-compiler –save-dev 安装 再到 webpack.config.js里边进行配置 plugin 在js文件里添加版权信息，webpack.BannerPluginconst webpack = require(&apos;webpack&apos;) module.exports = { plugins: [ new webpack.BannerPlugin(&apos;最终版权归xxx所有&apos;) ] } HtmlWebpackPlugin 插件 在dist文件（我们发布的文件）里边自动生成一个index.html文件（可以使用指定模板） 将打包的标签自动添加到body中的script标签当中 安装 npm install html-webpack-plugin --save-dev 使用 const HtmlWebpackPlugin = require(&apos;html-webpack-plugin&apos;) module.exports = { plugins: [ new webpack.BannerPlugin(&apos;最终版权归xxx所有&apos;) new HtmlWebpackPlugin() //也可以有参数 new HtmlWebpackPlugin({ template: &apos;index.html&apos; //根据index.html来生成，他会在当前的配置文件webpack.config.js所在的目录下找index.html，然后作为模板来生成 }) ] } js压缩的plugin uglifyjs-webpack-plugin webpack-dev-server搭建本地服务器 这个本地服务器基于node.js，内部使用express框架 安装： npm install webpack-dev-server –save-dev 配置： module.exports = { devServer: { contentBase: &apos;./dist&apos;, //要服务于那个文件夹 inline: true //是否实时刷新 port: 1234 //还可以配置端口，默认为8080 } } 使用： 在package.json的scripts里边加上 &quot;scripts&quot;: { &quot;serve&quot;: &quot;webpack-dev-server --open&quot; //--open的作用是在跑完webpack-dev-server后，自动打开网页 } 给文件夹起别名 在webpack.base.config.js里边的resolve属性的alias属性里边起别名 resolve: { alias: { &apos;@&apos;: resolve(&apos;src&apos;) //相当于给src起了别名为@，当我们使用/src/name时可以使用@/name } } 在组件的script里边可以直接使用 import aaa from ‘@/name’ 注意：当我们的路径在dom里边使用，比如img里边，因为是src，所以不能直接使用，要使用~ &lt;img src=&quot;~@/name&quot;&gt; 9. Vue CLI 安装 现在一般情况是使用的都是3.x的版本，通过 npm install @vue/cli -g来安装 如果即想用2的模板进行创建，也想用3的，就需要拉取2的模板，使用 npn install @vue/cli-init -g来进行安装 Vue CLI2初始化项目 指令： vue init webpack myProject 配置项： Project name: package.json中的项目的名称，直接敲回车的话，默认为初始化的名字 Project description: 项目描述，保存在package.json中 Author Vue build: 使用哪个版本的vue Runtime-only版本不能使用template，可能会导致项目不能运行， 优点：它更小，运行效率更高，性能更高 Runtime + Compiler 适合大多数使用者 区别： main.js里边有区别，compiler使用了template，runtime-only使用了render箭头函数，自动生成时为h函数(本质为createElement函数，参数有三个(‘标签名’，{标签的属性}, [标签的内容])) createElement(&apos;div&apos;, {class: &apos;box&apos;}, [&apos;hello&apos;, createElement(...)]) //内容里边可以继续嵌套 createElement(cpn) //也可以直接放入一个组件效果一样，但是因为省略的ast步骤，效率更高 - compiler里边的vue的运行过程：template -&gt; ast(abstract syntax tree抽象语法树) -&gt; render函数 -&gt; virtual DOM -&gt; real DOM(UI) - runtime-only的运行过程为： render函数 -&gt; virtual DOM -&gt; real DOM(UI) - 对比的话，only的性能更高，vue内部的代码量更少（因为它不需要写template -&gt; ast(abstract syntax tree抽象语法树) -&gt; render函数的处理的代码），runtime-only的.vue文件里边的template不需要使用template -&gt;ast，而是由vue-template-compiler这个库自己帮我们解析成了render函数 + vue-router: + ESLint: 是否使用ESLint对代码进行规范（ESLint当代码不规范时，编译直接报错） + unit test: 单元测试 + e2e test with Nightwatch: end to end 使用Nightwatch进行端到端测试 - **package-lock.json**：记录node_modules里安装的真实版本，在package.json（这里只会规定一个大概的版本）里边有时候要求的安装版本与node_modules里安装的会不同，这里进行记录 - static文件：在build后，会原封不动的复制到dist文件当中 Vue CLI3初始化项目 指令： vue create myProject 配置项(按空格是选择或者取消)： please pick a preset default(babel, ESlint) Manually 自己看吧。。。 public文件夹：在build后，原封不动的复制到dist当中 Vue CLI3的配置文件的修改方式： Vue UI：图形化界面进行修改 node_modules/@vue/cli-service/webpack.config.js，进去你会发现一个require(‘./lib/Service’)你会发现各种各样的配置 vue.config.js：在你当前这个项目的目录下创建这个文件，名字是固定的，在build时会进行合并 箭头函数 如果箭头函数中只有一行代码，那么它会自动把这行代码返回 箭头函数中this与正常的function定义不同 10. Vue-router SPA页面（单页面富应用， simple page web application，前端路由阶段） 核心为:改变url整体页面不刷新 如何实现改变url，页面不刷新 更改url的hash，例如： www.baidu.com console: location.hash = &apos;aaa&apos; result: www.baidu.com/aaa，但是页面不会刷新 H5里边的history模式: history.pushState({}, ‘’, ‘aaa’)history.back()history.forward()这两个可以配套，可以返回history.go() 注：history.go(-1) == history.back(),history.go(1) == history.forward() history.replaceState({}, ‘’, ‘bbb’) 这个不能返回 router-link to属性： 定义要指向的url &lt;router-link to=&apos;/home&apos;&gt;&lt;/router-link&gt; tag: router-link最终默认渲染成a标签，我们可以手动改为别的 &lt;router-link tag=&apos;div&apos; or tag=&apos;button&apos;&gt;&lt;/router-link&gt; router-link当使用h5的history时，默认使用pushState(可以使用网页中的后撤键)，我们可以手动设置成replaceState(禁止用户使用后撤键) &lt;router-link to=&apos;/home&apos; tag=&apos;li&apos; replace&gt;&lt;/router-link&gt; active-class:两个roter处于活跃状态时，添加的class(很少用) &lt;router-link to=&apos;/home&apos; tag=&apos;li&apos; replace active-class=&apos;myClass&apos;&gt;&lt;/router-link&gt; 在组件当中使用时，是使用$router属性进行修改 this.$router.push(&apos;/home&apos;) 动态路由的使用：例如，要在home页面后边动态的拼接url，就需要在映射里边进行修改 routes: [ { path: &apos;/home/:userId&apos; //这个userId可以为任意名字 component： Home } ] 然后在调用的时候用v-bind进行绑定就行 &lt;router-link :to=&quot;&apos;/home&apos; + 我们data里边的数据名称&quot;&gt;&lt;/router-link&gt; 我们如何获取home后边的数据，并使用到我们的vue当中：使用this.$route 可以在methods或者computed里边 this.$route.params.userId //这里为什么是userId，是因为上边我们写的名字为userId，加入是abc，这里就是abc this.$router和this.$route的区别： + this.$router:是我们在router.js里边我们const rou = new Router({})的常量 + this.$route是当前我们在使用哪个路由，也就是哪个路由处于活跃状态 路由的懒加载 vueCLI3，经过npm run build打包之后的js文件分为三个： app.哈希码.js：业务逻辑代码，也就是我们自己写的代码 manifest.哈希码.js：底层的支撑，比如说对ES6，CommonJS的支撑等 vendor.哈希码.js：项目里边引用的第三方的东西，例如vue和vue-router等 为什么使用懒加载 项目在打包时，js文件可能会非常大，同时加载可能会影响加载速度，所以需要懒加载（用到时再加载） 如果把不同的路由对应的组件分割成不同的代码块，当路由被访问时再加载对应的组件，效率就更好了（也就是用到时再加载） 懒加载做的事情就是将路由对应的组件包装成一个又一个的小的js文件，用到时再加载 代码实现： 原来是： import Home from &apos;./components/home&apos; routes: [ { path: &apos;/home&apos; component： Home } ] 懒加载： routes: [ { path: &apos;/home&apos; component： () =&gt; import(&apos;./components/home&apos;) } ] 或者(这样写的话更简洁，方便统一管理)： const Home = () =&gt; import(&apos;./components/home&apos;) routes: [ { path: &apos;/home&apos; component： Home } ] 路由的嵌套： 假如我们想在/home后边添加子路由/news，需要这样配置(子路由里边也可以设置默认值，设置redirect)： routes: [ { path: &apos;/home&apos; component： () =&gt; import(&apos;./components/home&apos;) children: [ { path: &apos;news&apos; //注意，这里不需要加/ component: import(&apos;./components/news&apos;) }, ] } ] 然后需要到Home组件当中再添加一个router-view，并且router-link也要注意写全，而不是只写news &lt;router-link to=&apos;/home/news&apos;&gt;&lt;/router-link&gt; &lt;router-view&gt;&lt;/router-view&gt; 路由的参数传递 第一种方式就是上边的动态路由 query方式 要更改router-link中的to属性，用v-bind绑定，然后绑定一个{}对象，里边有path和query属性 &lt;router-link :to=&quot;{path: &apos;/user&apos;, query:{name:&apos;lalala&apos;, age: 18}}&quot;&gt;&lt;/router-link&gt; 然后在user.vue里边使用$route.query.age等来取出来进行使用 在组件的methods中使用，也是使用this.$router.push()，然后在push里边输入上边的对象即可 methods: { 这是我的方法名() { return this.$router.push({ path: &apos;/user&apos;, query: { name:&apos;lalala&apos;, age: 18 } }) } } 导航守卫（去看看vue-router的官网） keep-alive及其他问题 vue组件除了created和beforeMouted等属性，还有activated和deactivated属性，不过这两个属性只有在组件被keep-alive包起来时才能使用 它有两个非常重要的属性： include：字符串或正则表达式，只有匹配的组件会被缓存 exclude：字符串或正则表达式，任何匹配的组件都不会被缓存 11. $符号的解释，例如$router 所有的vue组件都继承自vue类的原型 例如，当你给vue类的原型，添加一个属性时，那么所有的组件都可以使用这个属性 Vue.prototype.test = function(){ console.log(&apos;这是测试&apos;) } 那么你在任意的组件里边都可以使用this.test()进行调用，一般来说都使用$test来防止冲突 12. Promise Promise的基本使用： 什么情况下会用到promise？ 一般情况下，是有异步操作时，使用promise对异步操作进行封装 注意： Promise接收的参数是一个函数 resolve和reject都是函数 then和catch接收的也都是函数 链式调用使用方式： new Promise((resolve, reject) =&gt; { setTimeout((data) =&gt; { //这里用setTimeout来模拟异步操作 resolve(data) //用resolve函数来把data传给then函数 reject(&apos;error data&apos;) //数据获取失败，使用reject }, 1000) }).then(data =&gt; { //当数据获取成功时，使用resolve函数，然后执行then函数 console.log(data) return new Promise((resolve, reject) =&gt; { //来这里进行嵌套调用 }) }).catch(data =&gt; { console.log(data) //这里打印的就是error data这个字符串 }) Promise的链式调用 简化过程： new Promise((resolve, reject) =&gt; { setTimeout((data) =&gt; { resolve(data) reject(&apos;error data&apos;) }, 1000) }).then(data =&gt; { console.log(data) //拿到data后，想对data进行操作，比如拼接&apos;111&apos; return new Promise((resolve, reject) =&gt; { //这里的reject没用到，所以可以省略，写成resolve =&gt; {} resolve(data + &apos;111&apos;) }) }).then(res =&gt; { console.log(res) }) 上边的过程进行简化(Promise提供了resolve方法，当然也有reject方法，可以在catch里边使用) new Promise((resolve, reject) =&gt; { setTimeout((data) =&gt; { resolve(data) reject(&apos;error data&apos;) }, 1000) }).then(data =&gt; { console.log(data) return Promise.resolve(data + &apos;111&apos;) }).then(res =&gt; { console.log(res) }) 再次简化(Promise自动对返回的数据使用Promise.resolve进行封装，reject不行) new Promise((resolve, reject) =&gt; { setTimeout((data) =&gt; { resolve(data) reject(&apos;error data&apos;) }, 1000) }).then(data =&gt; { console.log(data) return data + &apos;111&apos; }).then(res =&gt; { console.log(res) }) 13. axios 方法简介 axios(config) axios({ url: params: method: timeout: ... }) axios.get() axios.post() axios.delete() axios.head() axios.put() axios.patch() 并发请求 做一个功能，需要两个请求都到达后，才向下进行 axios.all(list)： 参数：需要传入一个数组，需要几个同时到达，就在数组里边写几个数据请求 返回：一个数组 axios.all([axios(), axios()]).then(res =&gt; { res[0].操作 res[1].操作 }) 还有一个方法axios.spread可以将结果展开 axios.all([axios(), axios()]) .then(axios.spread((res1, res2) =&gt; { res1.操作 res2.操作 })) 关于配置config的相关信息 axios.default.baseURL巴拉巴拉的 baseURL timeout method transformRequest transformResponse headers params 创建实例 为了防止总是使用全局的axios产生混乱，或者处理不同请求的不同配置，需要为请求创建实例 axios.create(config) const instance1 = axios.create({ baseURL: timeout: }) instance1({ url: &quot;&quot; }).then(res =&gt; {}).catch(err =&gt; {}) //catch捕捉异常 拦截器 用于我们发送请求后，或者得到响应后，进行对应的处理 请求拦截:例如有一个instance1实例，那么拦截器代码为： instance1.interceptors.request.use(config =&gt; {}, err =&gt; {}) 拦截请求 use()代表你要使用这个拦截器，他有两个参数，两个参数都为函数 config:请求成功时使用的函数 err: 请求失败时的函数 使用： instance1.interceptors.request.use(config =&gt; { console.log(config) return config // **当你使用拦截器时，一定要返回，否则就被拦截掉了，无法请求到数据** }, err =&gt; {}) 请求拦截的作用： 比如我们在请求中，想添加那个圆圈动画，可以在config中添加， 或者对config进行规范，这就是拦截器的作用 比如我们请求数据时，需要携带token，比如需要用户先登录，就可以先拦截下来，然后跳转到用户登录界面，进行登录，再继续请求数据 响应拦截： instance1.interceptors.response 拦截响应 使用： instance1.interceptors.response().use(res =&gt; { 一系列处理。。。。 return res //记得返回，否则请求的地方就拿不到结果 或者： return res.data //只返回data，这样可以去掉服务器给我们的其他的乱七八糟的东西 }, err =&gt; {}) 14. Vuex Vuex简介 什么是Vuex？ Vuex是专为Vus.js开发的一个状态管理模式 所谓状态管理模式就是，有很多个组件需要共享多个变量，那么把这多个变量放到一个对象里边，然后把这个对象放到vue的顶层的示例里边，给所有组件共享（我们也可以自己写，但是Vuex是响应式的，自己实现比较麻烦）。 Vuex里边储存的一定是需要在多个页面共享的状态 什么样的状态需要在多个组件之间共享，而不是使用父子组件传讯的方式？（token） 大型项目里边的状态，比如说：用户的登陆状态、地理位置信息、购物车状态等，这些状态就需要统一的地方保存和管理，并且还是响应式的 token：在后台请求数据时，需要携带token（令牌） Vuex的使用 创建过程(真正创建的不是new Vuex而是 new Vuex.Store)： import Vue from &apos;vue&apos; import Vuex from &apos;vuex&apos; Vue.use(Vuex) const store = new Vuex.Store({ state:{}, //存储数据状态 mutations:{}, //定义方法， actions:{}, //用来处理异步操作 getters:{}, //类似于组件里边的计算属性 modules:{} //划分模块，针对不同的模块再进行管理 }) export default store &lt;然后再main class=”js里边导入并注册”&gt;&lt;/然后再main&gt; state的修改过程： Vue components -(dispathc)-&gt; Actions -(commit)-&gt; Mutations -(mutate)-&gt; State -(Render)-&gt; Vue components 或者Vue components –&gt; Mutations –&gt; State –&gt; Vue components Actions:它的作用是用来处理异步操作，异步网络请求，因为为了追踪State是在哪里被修改了，官方给Mutations开发了一个Chrome插件Devtools，而这个插件只能监听同步操作，无法监听异步，所以才需要Actions 核心模块详解： state：保存状态，推荐使用单一状态树，即只创建一个store 单一状态树：就是建议在store.js里边只有一个new store getters：类似于组件里边的计算属性computed，它也有默认的state参数，还可以有另外一个参数就是getters，也就是这个getters本身，用来获取getters里边的方法，例如sum(state, getters)，也就是说，不管你前两个参数叫什么名字，第一个参数代表的一定是state，第二个一定是getters，所以说如果想给getters传参，要使用其他方法： getters: { sum(state) { return function(这是我想传入的参数){ //在这里使用参数即可 } } } 因为返回的是个函数，所以在调用的时候以函数的形式使用，并传入参数即可 this.$store.getters.sum(我是参数) mutations：这里边的方法会有一个默认参数为state，也就是上边这个state，比如说定义sum(),它相当于sun(state) 使用mutations里边定义的函数的方法： store.commit(&apos;mutations里边定义的方法名&apos;) components里边为this.$store.commit(&apos;sum&apos;) 传参方式： 首先： sum (state, num){ //在这里定义并使用 sum += num } store.commit(&apos;sum&apos;, count) //先这里进行传参 另外一种提交方式： store.commit({ type: &apos;sum&apos;, count: count }) 使用这种方式的时候就要注意，传过去的参数，不是一个数值，而是一个对象，在函数里边使用时，要用num.count才可以 类型常量（去看一眼视频吧，理解了下边挺简单的，或者看官网)：就是单独创建了一个文件，在里边定义函数名并导出，然后后按照第二种方法的声明方式，在mutations里边和使用的地方进行统一，例如 export const MYPARAM = &apos;myparam&apos; import {MYPARAM} from &apos;xx.js&apos; 原来函数定义为： myparam(){ } 因为可以定义为 [&apos;myparam&apos;](){ } 所以现在可以为： [MYPARAN](){ } 这样的话，在使用的时候，方法名得到了统一，哪怕我在export const MYPARAM = ‘myparam’写错了，对我的代码也没有影响，这是官方推荐形式 方法的定义不光可以使用： 方法名(){ } 也可以使用 [&apos;方法名&apos;](){ } action：因为Devtool不能监听异步操作，所以所有异步操作都要在action里边进行 函数声明和mutations一样，只不过默认参数不一样： sum(context, param){ //这里的param是用来接收参数，没有传参可以不写，这里的context可以看成我们声明的store，也就是$store setTimeout(() =&gt; { //这里调用mutations里边的函数！注意这里不能跳过mutations直接操作state context.state.myData = &apos;lalalal&apos; //直接修改了所以是错误的 context.commit(&apos;mutations里边的方法&apos;) //正确写法 }, 1000) } 在使用时，应该是 this.$store.dispatch(&apos;sum&apos;, &apos;这第二个参数，是我要传递的参数，可以不写&apos;) 如何判断里边的异步函数已经完成，可以从action里边传回一个Promise(可以看看视频，我觉得天秀)： sum(context, myParam){ return new Promise((resolve, reject) =&gt; { setTimeout(() =&gt; { context.commit(&apos;mutations里边的方法&apos;) console.log(myParam) resolve(&apos;这是我想传回去的参数&apos;) }, 1000) //神奇的地方来了，不在这里写then，而是在调用的地方 }) } this.$store.dispatch(&apos;sum&apos;, &apos;这是参数&apos;).then(res =&gt; { console.log(&apos;里边的异步已经执行完了&apos;) console.log(res) //这是resolve给的数据 }) 解释：因为then函数是接在new Promise后边的，而sum函数return的就是一个new Promise，所以我们的this.$store.dispatch(‘sum’, ‘这是参数’)这句代码执行完之后，相当于被替换成了new Promise()，所以它后边可以接then（天秀！！！） modules：当你想要进行模块划分时，在这里边进行，举例： const moduleC = { state:{}, mutations:{}, actions:{}, getters:{}, modules:{} } modules: { a: { state:{}, mutations:{}, actions:{}, getters:{}, modules:{} }, b: { state:{}, mutations:{}, actions:{}, getters:{}, modules:{} }, c: moduleC } 这样来进行你想要的划分，使用方法： state：modules里边的state在使用时，Vue是将它封装成一个对象，放在根state里边的，所以使用是： $store.state.a.name mutations：modules里边的mutations的命名不能与根store里边的命名重复，因为它的使用是：$store.commit(‘modules里边的mutations的方法名’)，直接调用，如果重复了会有冲突 getters：modules里边的getters，可以直接调用自己本模块的getters，也可以直接调用根模块的getters actions：与根模块相比，他的context多了两个属性：rootGetters和rootState 对象的解构： const obj = { name: &apos;123&apos;, age: 19, height:1.88 } 解构的使用办法为： const {name, height, age} = obj //它是按名字取值的，不是按顺序 目录结构： 将mutations和actions等，抽成单个的文件，然后再store.js里边导入 Vuex的数据响应式 什么样的数据是响应式 必须是提前在store中初始化好的属性，也就是说我们后边通过一些方法添加的属性，不是响应式的，例如: store.state.info[&apos;address&apos;] = &apos;lalala&apos; //address是我们新添加的属性 我们可以使用Vue的方法来使我们添加的数据变为响应式 Vue.set(要更改的数据, 下标或属性, value) //当要改的数据为数组时，第二个参数为下标 Vue.delete(要更改的数据, 下标或属性)","categories":[],"tags":[]},{"title":"V 开场自我介绍","slug":"V开场台词","date":"2020-02-17T06:20:11.000Z","updated":"2020-02-17T06:21:59.631Z","comments":true,"path":"2020/02/17/V开场台词/","link":"","permalink":"http://yoursite.com/2020/02/17/V%E5%BC%80%E5%9C%BA%E5%8F%B0%E8%AF%8D/","excerpt":"","text":"台词 V: I can assure you,I mean you no harm 女主: Who are you? V: Who?Who is but the form following the function of what and what I am is a man in a mask 女主: Oh,I can see that V: Of course you can.I’m not questioning your powers of observation.I’m merely remarking upon the paradox of asking a masked man who he is. 女主: Right V: But on this most auspicious of nights permit me then,in lieu of the more commonplace sobriquet to suggest the character of this dramatis persona. V: Voila! In view,a humble vaudevillian veteran cast vicariously as both victim and villain by the vicissitudes of fate. This visage, no mere veneer of vanity is a vestige of the vox populi, now vacant, vanished. However, this valorous visitation of a bygone vexation stands vivified and has vowed to vanquish these venal and virulent vermin vanguarding vice and vouchsafing the violently vicious and voracious violation of volition. The only verdict is vengeance, a vendetta held as a votive not in vain, for the value and veracity of such shall one day vindicate the vigilant and the virtuous. Verily, this vichyssoise of verbiage veers most verbose. So let me simply add that it’s my very good honor to meet you and you may call me V. 哇啦！ 我虽然看起来像个小丑，受到命运残酷的作弄，不得不戴上面具，被迫昼伏夜出，不见天日，但我仍不畏强权，挺身而出，发誓铲奸除恶，伸张正义，为饱受压迫的人民出一口气，唯一的方法就是复仇，这不是戏言，而是誓言，想要拯救水深火热中的同胞，就要使用以暴制暴的极端手段.我的冗长赘言就此结束，最后容我说，很荣幸认识你，你可以叫我V。","categories":[{"name":"英语","slug":"英语","permalink":"http://yoursite.com/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"电影台词","slug":"电影台词","permalink":"http://yoursite.com/tags/%E7%94%B5%E5%BD%B1%E5%8F%B0%E8%AF%8D/"}]},{"title":"AdvancedAutoEncoder","slug":"AdvancedAutoEncoder","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:49.804Z","comments":true,"path":"2019/12/23/AdvancedAutoEncoder/","link":"","permalink":"http://yoursite.com/2019/12/23/AdvancedAutoEncoder/","excerpt":"","text":"1. 简介这种方法是在编码阶段引入噪声来增强自编码器的鲁棒性，称为去噪编码器，也被称为“随机版本的自编码器”，其中的输入是随机损坏的，而生成同一个输入的未损坏版本是解码阶段的目标。直观来说，去噪编码器要完成两个任务： 对输入编码，并保留主要信息 降低甚至避免损坏输入的影响 2. 代码简介 结果展示函数 def plotresult(org_vec,noisy_vec,out_vec): plt.matshow(np.reshape(org_vec, (28, 28)), cmap=plt.get_cmap(&quot;gray&quot;)) plt.title(&quot;Original Image&quot;) plt.colorbar() plt.matshow(np.reshape(noisy_vec, (28, 28)), cmap=plt.get_cmap(&quot;gray&quot;)) plt.title(&quot;Input Image&quot;) plt.colorbar() outimg = np.reshape(out_vec, (28, 28)) plt.matshow(outimg, cmap=plt.get_cmap(&apos;gray&apos;)) plt.title(&quot;Reconstructed Image&quot;) plt.colorbar() plt.show() 定义了一个函数来对最终的结果进行展示，有关matplotlib.pyplot这个库我没详细的看，总的来说就是展示结果用的，它会显示原始图像，噪声图像和预测图像 参数定义 n_input = 784 n_hidden_1 = 256 n_hidden_2 = 256 n_output = 784 epochs = 110 batch_size = 100 disp_step = 10 这边参数的定义与自编码器差不多，disp_step这个参数是用来在控制台打印cost用的，下边我会标注 mnist = input_data.read_data_sets(&quot;MNIST_Labels_Images&quot;,one_hot=True) trainimg = mnist.train.images trainlabel = mnist.train.labels testimg = mnist.test.images testlabel = mnist.test.labels 这边它对于训练图片，训练标签以及测试图片和测试标签单独定义了变量，只在展示结果，也就是plt中用到了，训练过程中没有用到 weights = { &apos;h1&apos;: tf.Variable(tf.random_normal([n_input, n_hidden_1])), &apos;h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_2, n_output])) } biases = { &apos;b1&apos;: tf.Variable(tf.random_normal([n_hidden_1])), &apos;b2&apos;: tf.Variable(tf.random_normal([n_hidden_2])), &apos;out&apos;: tf.Variable(tf.random_normal([n_output])) } encode_in = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;h1&apos;]), biases[&apos;b1&apos;])) encode_out = tf.nn.dropout(encode_in, dropout_keep_prob) decode_in = tf.nn.sigmoid(tf.add(tf.matmul(encode_out, weights[&apos;h2&apos;]), biases[&apos;b2&apos;])) decode_out = tf.nn.dropout(decode_in, dropout_keep_prob) y_pred = tf.nn.sigmoid(tf.add(tf.matmul(decode_out, weights[&apos;out&apos;]), biases[&apos;out&apos;])) cost = tf.reduce_mean(tf.pow(y_pred - y, 2)) optmizer = tf.train.RMSPropOptimizer(0.01).minimize(cost) 这边的定义与自编码器也大体相同，只有一点区别就是使用了dropout优化 dropout优化介绍 介绍：在学习阶段，网络层与下一层的连接可以限制为神经元的子集，以减少需要更新的权重数量，这种学习优化技术成为dropout优化。 作用：可以减少层数较多&nbsp;和\\或&nbsp;神经元数量较多的网络的过拟合问题 dropout层位置：dropout层一般置于训练神经元较多的网络层之后 工作机制简介：每个隐藏单元都以概率p被随机从网络中忽略 训练过程 with tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) print(&quot;Start Training&quot;) for epoch in range(epochs): num_batch = int(mnist.train.num_examples/batch_size) total_cost = 0 for i in range(num_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs_noisy = batch_xs + 0.3*np.random.randn(batch_size, 784) feeds = {x: batch_xs_noisy, y: batch_xs, dropout_keep_prob: 0.8} sess.run(optmizer, feed_dict=feeds) total_cost = sess.run(cost, feed_dict=feeds) if epoch % disp_step == 0: print(&quot;Epoch %02d/%02d average cost: %.6f&quot; % (epoch, epochs, total_cost/num_batch)) 大部分代码还是TensorFlow的常规代码，最后一个if里边用到了disp_step，他只是展示结果用的不重要。 最重要的代码是嵌套的for循环里边的几句： batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs_noisy = batch_xs + 0.3*np.random.randn(batch_size, 784) feeds = {x: batch_xs_noisy, y: batch_xs, dropout_keep_prob: 0.8} sess.run(optmizer, feed_dict=feeds) total_cost = sess.run(cost, feed_dict=feeds) 第二行，使用numpy包中的random函数，随机损坏batch_xs数据集，然后使用被破坏的数据集进行 randidx = np.random.randint(testimg.shape[0], size=1) orgvec = testimg[randidx, :] testvec = testimg[randidx, :] label = np.argmax(testlabel[randidx, :], 1) print(&quot;Test label is %d&quot; % (label)) noisyvec = testvec + 0.3*np.random.randn(1, 784) outvec = sess.run(y_pred, feed_dict={x: noisyvec, dropout_keep_prob: 1 }) plotresult(orgvec,noisyvec,outvec) 这里是为了测试该模型，随机从测试集中选取一个图像，并在选出的图像上运行训练出的模型，然后plotresult来显示原始图像、噪声图像和预测图像","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AutoEncoder","slug":"AutoEncoder","permalink":"http://yoursite.com/tags/AutoEncoder/"}]},{"title":"AutoEncoder","slug":"AutoEncoder","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:50.395Z","comments":true,"path":"2019/12/23/AutoEncoder/","link":"","permalink":"http://yoursite.com/2019/12/23/AutoEncoder/","excerpt":"","text":"1. 简介 作用：自编码器可以帮助我们将高维数据压缩到我们所希望的维度 实现原理：先对原数据进行压缩降维，再通过相反的过程解码得到结果，然后原原数据进行比较，通过修正权重偏置参数降低损失函数，提高对原数据的复原能力 大概可能利用的地方：可以通过前半段的编码过程得到可代表原数据的低维的“特征值” 2. 代码解释 导入包数据，并且设置超参数 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.examples.tutorials.mnist import input_data # 导入MNIST数据 mnist = input_data.read_data_sets(&quot;MNIST_Labels_Images&quot;, one_hot=False) # 设置训练超参数 learning_rate = 0.01 # 学习速率 training_epochs = 10 # 训练轮数 batch_size = 256 # 训练批次大小 display_step = 1 # 显示间隔 examples_to_show = 10 # 表示从测试集中选择十张图片去验证自动编码器的结果 n_input = 784 # 数据的特征值个数 特征值为784的原因：因为MNIST的图片像素为为28*28=784，换算成一维数组就是784 X = tf.placeholder(&quot;float&quot;, [None, n_input]) x不是一个特定的值，而是一个占位符placeholder，用2维的浮点数张量来表示这些图，这个张量的形状是[None，784 ]。（这里的None表示此张量的第一个维度可以是任何长度的。） 创建隐藏层和生成参数矩阵 # 用字典的方式存储各隐藏层的参数 网络参数 n_hidden_1 = 256 # 第一编码层神经元个数256，也是特征值个数 n_hidden_2 = 128 # 第二编码层神经元个数128，也是特征值个数 # 权重参数矩阵维度是每层的 输入*输出，偏置参数维度取决于输出层的单元数 weights = { &apos;encoder_h1&apos;: tf.Variable(tf.random_normal([n_input, n_hidden_1])), &apos;encoder_h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), &apos;decoder_h1&apos;: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])), &apos;decoder_h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_input])), } biases = { &apos;encoder_b1&apos;: tf.Variable(tf.random_normal([n_hidden_1])), &apos;encoder_b2&apos;: tf.Variable(tf.random_normal([n_hidden_2])), &apos;decoder_b1&apos;: tf.Variable(tf.random_normal([n_hidden_1])), &apos;decoder_b2&apos;: tf.Variable(tf.random_normal([n_input])), } 这个神经网络一共为5层，2层编码层，2层解码层和1个输出层 第一编码层神经元个数256，第二编码层神经元个数128，解码层与编码层是相逆的,所以解码层1层为128，2层为256 权重参数矩阵维度是每层的 输入*输出，偏置参数维度取决于输出层的单元数，通过字典的方式进行保存 random_normal是从正态分布中输出随机值，第一个参数为Shape 构建编码器和解码器 # 定义有压缩函数，每一层结构都是 xW + b # 构建编码器 def encoder(x): layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;encoder_h1&apos;]), biases[&apos;encoder_b1&apos;])) layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&apos;encoder_h2&apos;]), biases[&apos;encoder_b2&apos;])) return layer_2 # 定义解压函数 # 构建解码器 def decoder(x): layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;decoder_h1&apos;]), biases[&apos;decoder_b1&apos;])) layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&apos;decoder_h2&apos;]), biases[&apos;decoder_b2&apos;])) return layer_2 # 构建模型 encoder_op = encoder(X) decoder_op = decoder(encoder_op) 自编码器的神经网络结构非常有规律性，都是xW + b的结构，每一层的激活函数使用Sigmoid函数 这一行代码： layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;encoder_h1&apos;]), biases[&apos;encoder_b1&apos;])) - tf.matmul()&amp;nbsp;&amp;nbsp;矩阵的乘法运算 - tf.add(a,b)&amp;nbsp;&amp;nbsp;a+b - tf.nn.sigmoid 函数：f(z) = 1 / (1 + exp( − z))&amp;nbsp;&amp;nbsp;&amp;nbsp;这里z为xW + b - 所以这一句的意思就是&amp;nbsp;(None行784列) \\* (784行256列)&amp;nbsp;得到None行256列 - 将输入层784个特征压缩到256个 - 下边一行同理，将256压缩到128 这一行解码层的代码： layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;decoder_h1&apos;]), biases[&apos;decoder_b1&apos;])) - 因为在构建模型时传入的是压缩之后的特征，所以这句代码就是&amp;nbsp;(None行128列) \\* (128行256列)&amp;nbsp;得到不定行256列，将编码层第2层层128个特征解压到256个 - 下边一行同理，将256解压到784 预测、代价函数及优化器 # 预测 y_pred = decoder_op # 得出预测值 y_true = X # 输入值 即得出真实值 # 定义代价函数和优化器 cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # 最小二乘法 平方差取平均 optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # 优化器采用 AdamOptimizer 或者 RMSPropOptimizer 通过神经网络得到的复原结果为预测值，输入为真实值x tf.reduce_mean()&nbsp;&nbsp;求平均值 优化器使用的是TensorFlow提供的优化器，去最小化损失函数 训练数据和评估模型 with tf.Session() as sess: # tf.initialize_all_variables() no long valid from # 2017-03-02 if using tensorflow &gt;= 0.12 # 检测当前TF版本 if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12 and int((tf.__version__).split(&apos;.&apos;)[0]) &lt; 1: init = tf.initialize_all_variables() else: init = tf.global_variables_initializer() sess.run(init) # 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练 total_batch = int(mnist.train.num_examples / batch_size) # 总批数 for epoch in range(training_epochs): for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # max(x) = 1, min(x) = 0 # Run optimization op (backprop) and cost op (to get loss value) _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs}) if epoch % display_step == 0: print(&quot;Epoch:&quot;, &apos;%04d&apos; % (epoch + 1), &quot;cost=&quot;, &quot;{:.9f}&quot;.format(c)) print(&quot;Optimization Finished!&quot;) 首要要搞清楚一点，TensorFlow使用图 (graph) 来表示计算任务，在被称之为 会话 (Session) 的上下文 (context) 中执行图. 这个代码首先检查了TensorFlow的版本，因为不同版本初始化的代码有所改动 生成会话之后，所有tf.Variable实例都会立即通过调用各自初始化操作中的sess.run()函数进行初始化。 total_batch：&nbsp;他把训练样本划分为了不同的批次，用&nbsp;样本总数&nbsp;/&nbsp;训练批次大小&nbsp;每个批次的数据只有batch_size=256个(这是最开始定义的超参数)， 使用双循环嵌套，第一个表示训练的总次数，第二个用来控制训练批次，每次只是用一个批次的数据 这行代码： batch_xs, batch_ys = mnist.train.next_batch(batch_size) # max(x) = 1, min(x) = 0 mnist.train.next_batch()&nbsp;获取下一个批次的图片和标签，分别保存到batch_xs, batch_ys 这行代码： _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs}) 这里只使用了图片，而未使用标签，来对模型进行训练，因为optimizer在训练过程中不会产生输出，所以被舍弃，值保存了cost 最后通过循环，完成对模型的训练 对测试集应用训练网络 encode_decode = sess.run(y_pred, feed_dict={X: mnist.test.images[:examples_to_show]}) # 比较测试集原始图片和自动编码网络的重建结果 f, a = plt.subplots(2, 10, figsize=(10, 2)) for i in range(examples_to_show): a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28))) # 测试集 a[1][i].imshow(np.reshape(encode_decode[i], (28, 28))) # 重建结果 plt.show() 这时候我们就能够使用训练好的参数矩阵，也就是模型来做我们的试验了！！","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AutoEncoder","slug":"AutoEncoder","permalink":"http://yoursite.com/tags/AutoEncoder/"}]},{"title":"卷积自编码器","slug":"CNNAutoEncoder","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:51.881Z","comments":true,"path":"2019/12/23/CNNAutoEncoder/","link":"","permalink":"http://yoursite.com/2019/12/23/CNNAutoEncoder/","excerpt":"","text":"1. 简介 目的：卷积自编码器创建的目的就在于，利用卷积神经网络的卷积和池化操作，实现特征不变性提取（invariant feature）的无监督特征提取 作用：卷积自编码器可以用于图像的重构工作。例如，他们可以学习从图片中去除噪声，或者重构图片缺失的部分。 原理：这里的原理和卷积差不多，不过这里使用了反卷积操作 2. 代码 导入数据 mnist = imput_data.read_data_sets(&apos;MNIST_Labels_Images&apos;,one_hot=&quot;True&quot;) trainimgs = mnist.train.images trainlabels = mnist.train.labels testimgs = mnist.test.images testlabels = mnist.test.labels ntrain = trainimgs.shape[0] ntest = trainimgs.shape[0] dim = trainimgs.shape[1] nout = trainlabels.shape[1] print(&quot;Packages loaded&quot;) shape[0]是查看ndarray行数，shape[1]是查看列数，因为这个数据局一共有55000张图片，每个图片为28*28，所以shape[0]为55000，shape[1]为784 占位符和各层参数 n1 = 16 n2 = 32 n3 = 64 ksize = 5 x = tf.placeholder(tf.float32, [None, dim]) y = tf.placeholder(tf.float32, [None, dim]) keepprob = tf.placeholder(tf.float32) 定义每层卷积核的数量，以及尺寸，x为带有噪声的输入数据的占位符，y为原始输入数据的占位符，因为要防止过拟合，所以使用了dropout优化，所以设置了keepprob占位符 权重矩阵和偏置 weights = { &apos;ce1&apos;: tf.Variable(tf.random_normal([ksize, ksize, 1, n1], stddev=0.1)), &apos;ce2&apos;: tf.Variable(tf.random_normal([ksize, ksize, n1, n2], stddev=0.1)), &apos;ce3&apos;: tf.Variable(tf.random_normal([ksize, ksize, n2, n3], stddev=0.1)), &apos;cd3&apos;: tf.Variable(tf.random_normal([ksize, ksize, n2, n3], stddev=0.1)), &apos;cd2&apos;: tf.Variable(tf.random_normal([ksize, ksize, n1, n2], stddev=0.1)), &apos;cd1&apos;: tf.Variable(tf.random_normal([ksize, ksize, 1, n1], stddev=0.1)) } biases = { &apos;be1&apos;: tf.Variable(tf.random_normal([n1], stddev=0.1)), &apos;be2&apos;: tf.Variable(tf.random_normal([n2], stddev=0.1)), &apos;be3&apos;: tf.Variable(tf.random_normal([n3], stddev=0.1)), &apos;bd3&apos;: tf.Variable(tf.random_normal([n2], stddev=0.1)), &apos;bd2&apos;: tf.Variable(tf.random_normal([n1], stddev=0.1)), &apos;bd1&apos;: tf.Variable(tf.random_normal([1], stddev=0.1)) ‘ce1’:尺寸为5*5，通道为1，卷积核数量为n1，因为输入的数据通道为1，所以这里为1，而n1是第一层卷积层卷积核的数量，是由我们自己定义的 ‘ce2’:尺寸为5*5，因为上一层卷积层卷积核数量为n1，所以上一次的通道数就变成了n1，所以这里的通道数就为n1，第二层卷积层的卷积核数量为n2, ‘ce3’:与前两个相同 注意：卷积操作tf.nn.conv2d中的filter(过滤器)参数它的格式为[filter_height, filter_width, in_channels, out_channels]的形式，而反卷积tf.nn.conv2d_transpose中的filter参数，是[filter_height, filter_width, out_channels，in_channels]的形式，注意in_channels和out_channels反过来了！因为两者互为反向，所以输入输出要调换位置 由上可知’cd3’、’cd2’,’cd1’的定义 偏置的定义要与每层的输出通道相同。 定义神经网络 def cae(_X, _W, _b, _keepprob): _input_r = tf.reshape(_X, shape=[-1, 28, 28,1]) #Encoder _ce1 = tf.nn.sigmoid(tf.add(tf.nn.conv2d(_input_r, _W[&apos;ce1&apos;], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;be1&apos;])) _ce1 = tf.nn.dropout(_ce1, _keepprob) _ce2 = tf.nn.sigmoid(tf.add(tf.nn.conv2d(_ce1, _W[&apos;ce2&apos;], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;be2&apos;])) _ce2 = tf.nn.dropout(_ce2, _keepprob) _ce3 = tf.nn.sigmoid(tf.add(tf.nn.conv2d(_ce2, _W[&apos;ce3&apos;], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;be3&apos;])) _ce3 = tf.nn.dropout(_ce3 , _keepprob) #Decoder _cd3 = tf.nn.sigmoid(tf.add( tf.nn.conv2d_transpose(_ce3, _W[&apos;cd3&apos;], tf.stack([tf.shape(_X)[0], 7, 7,n2]),strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;bd3&apos;])) _cd3 = tf.nn.dropout(_cd3, _keepprob) _cd2 = tf.nn.sigmoid(tf.add( tf.nn.conv2d_transpose(_cd3, _W[&apos;cd2&apos;], tf.stack([tf.shape(_X)[0], 14, 14, n1]), strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;bd2&apos;])) _cd2 = tf.nn.dropout(_cd2, _keepprob) _cd1 = tf.nn.sigmoid(tf.add( tf.nn.conv2d_transpose(_cd2, _W[&apos;cd1&apos;], tf.stack([tf.shape(_X)[0], 28, 28, 1]), strides=[1, 2, 2, 1], padding=&apos;SAME&apos;), _b[&apos;bd3&apos;])) _cd1 = tf.nn.dropout(_cd1, _keepprob) _out = _cd1 return _out print(&quot;Network ready&quot;) pred = cae(x, weights, biases, keepprob) 这个函数输入的数据为(x, weights, biases, keepprob)，所以 _input_r = tf.reshape(_X, shape=[-1, 28, 28,1]) 这句话的意思就是改变输入数据的形状 编码层就是使用输入数据和每个编码层的权重矩阵进行卷积操作，激活函数为sigmoid函数，然后使用哪个dropout优化，防止过拟合 tf.nn.conv2d_transpose(value,filter,output_shape) tf.stack()是一个矩阵拼接函数，这里定义了反卷积函数输出的矩阵的形状 损失和优化 pred = cae(x, weights, biases, keepprob) cost = tf.reduce_sum( tf.square(cae(x, weights, biases, keepprob)) + tf.reshape(y, shape=[-1, 28, 28, 1])) learning_rate = 0.001 optm = tf.train.AdadeltaOptimizer(learning_rate).minimize(cost) init = tf.global_variables_initializer() print(&quot;Functions ready&quot;) saver = tf.train.Saver(max_to_keep=1) 定义损失函数，优化函数，定义初始化函数，定义保存函数，我将我运行的数据保存了，当再次运行时，可以选择直接运行已经跑完的module，也可以从头训练 训练过程 ef my_train(): with tf.Session() as sess: sess.run(init) mean_img = np.zeros((784)) batch_size = 128 n_epochs = 5 print(&quot;Strart training&quot;) for epoch_i in range(n_epochs): for batch_i in range(mnist.train.num_examples // batch_size): batch_xs, _ = mnist.train.next_batch(batch_size) trainbatch = np.array([img - mean_img for img in batch_xs]) #使batch_xs中的每个图像减去mean_img,并生成ndarray # trainbatch = batch_xs trainbatch_noisy = trainbatch + 0.3*np.random.randn(trainbatch.shape[0], 784) sess.run(optm, feed_dict={x: trainbatch_noisy, y: trainbatch, keepprob: 0.7}) #saver.save(sess, &apos;./test&apos;, global_step= epoch_i+1) print(&quot;[%02d/%02d] cost: %.4f&quot; % (epoch_i, n_epochs, sess.run(cost, feed_dict={x:trainbatch_noisy, y:trainbatch, keepprob: 1.}))) 训练过程和其他的差不多，里边有一句我不是很理解： trainbatch = np.array([img - mean_img for img in batch_xs]) #使batch_xs中的每个图像减去mean_img,并生成ndarray # trainbatch = batch_xs 它这里减去了一个自己定义的所有数据为0的ndarray，但是当我使用注释的那一句#trainbatch = batch_xs这一句，也就是使用它的原数据，没使用它的减掉ndarray这个操作，他们得到的cost是差不多的，我感觉没啥用- - 结果展示 if(epoch_i % 1) == 0: n_examples = 5 test_xs, _ = mnist.test.next_batch(n_examples) test_xs_noisy = test_xs + 0.3*np.random.randn(test_xs.shape[0], 784) recon = sess.run(pred, feed_dict={x: test_xs_noisy, keepprob: 1.}) fig, axs = plt.subplots(2, n_examples,figsize=(15, 4)) for example_i in range(n_examples): axs[0][example_i].matshow(np.reshape(test_xs_noisy[example_i, :],(28, 28)), cmap=plt.get_cmap(&apos;gray&apos;)) axs[1][example_i].matshow(np.reshape(np.reshape(recon[example_i, ...], (784,)) + mean_img, (28, 28)), cmap=plt.get_cmap(&apos;gray&apos;)) plt.show() 这部分属于plt库，这里总报错，我调 了好久也没解决，所以在代码中被我注释掉了，但是这个并不影响模型的结果，只是不能看见结果，我把书上的结果放到这里看一哈","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AutoEncoder","slug":"AutoEncoder","permalink":"http://yoursite.com/tags/AutoEncoder/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"卷积神经网络","slug":"CNN","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:51.189Z","comments":true,"path":"2019/12/23/CNN/","link":"","permalink":"http://yoursite.com/2019/12/23/CNN/","excerpt":"","text":"1. 简介 卷积神经网络的一般结构：conv-&gt;pool-&gt;conv-&gt;pool-&gt;fc-&gt;fc-&gt;fc-&gt;softmax conv：卷积层 pool：池化层 fc：全连接层 softmax：分类函数 各层的作用： 卷积层作用：可以使用卷积来提取自己想要的特征 池化层作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。主要分为两类 最大池化层：把每个区域的最大值输出 平均池化层：把每个区域的平均值输出 全连接层作用:连接所有特征，将输出值送给分类器 2. 代码 导入数据 #导入input_data用于自动下载和安装MNIST数据集 from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) #创建两个占位符，x为输入网络的图像，y_为输入网络的图像类别 x = tf.placeholder(&quot;float&quot;, shape=[None, 784]) y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10]) 导入MNIST数据集，并设置占位符来保存图片数据和预测信息 初始化函数 #权重初始化函数 def weight_variable(shape): #输出服从截尾正态分布的随机值 initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) #偏置初始化函数 def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 这里使用了函数来对权重和偏置进行初始化。 卷积层和池化层函数 #创建卷积op #x 是一个4维张量，shape为[batch,height,width,channels] #卷积核移动步长为1。填充类型为SAME,可以不丢弃任何像素点, VALID丢弃边缘像素点 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=&quot;SAME&quot;) #创建池化op #采用最大池化，也就是取窗口中的最大值作为结果 #x 是一个4维张量，shape为[batch,height,width,channels] #ksize表示pool窗口大小为2x2,也就是高2，宽2 #strides，表示在height和width维度上的步长都为2 def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&quot;SAME&quot;) 卷积层参数介绍： x为输入的数据，shape为[batch,height,width,channels]，batch的数量，高和宽，以及它的通道数，这里使用的数据集的图片通道是1 W为权重矩阵，这里的权重矩阵就是卷积核(过滤器)中的参数信息，举例：假如是5*5的过滤器，那么参数就有25个 strides是步长，可以使一个实数也可以是一个4维向量，第一和第四默认为1（不知道具体意思，官方文档也没有介绍，好像是和通道有关），第二个为横向步长，第二个为纵向步长，这里表示在height和width维度上的步长都为1 padding：有两个参数’same’和’valid’，’same’表示不舍弃边缘信息，’vaild’表示舍弃边缘信息. 池化层参数介绍： x，strides和padding与卷积层的作用一样，这里strides表示在height和width维度上的步长都为2 ksize表示的是池化层的大小，这里是高2，宽2 这里使用的是最大池化层，也就是去窗口中的最大值作为结果 第一层卷积层 #第1层，卷积层 #初始化W为[5,5,1,6]的张量，表示卷积核大小为5*5，1表示图像通道数，6表示卷积核个数即输出6个特征图 W_conv1 = weight_variable([5,5,1,6]) #初始化b为[6],即输出大小 b_conv1 = bias_variable([6]) #把输入x(二维张量,shape为[batch, 784])变成4d的x_image，x_image的shape应该是[batch,28,28,1] #-1表示自动推测这个维度的size x_image = tf.reshape(x, [-1,28,28,1]) #把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling #h_pool1的输出即为第一层网络输出，shape为[batch,14,14,6] h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1) 使用的卷积核(过滤器)是高宽5*5，通道1，一共有6个卷积核，偏置的数量要与卷积核数量相等，所以也是6 因为要求输入的数据是[batch,height,width,channels]，而数据集的数据x是[batch, 784]，所以要reshape。-1表示自动推测这个维度的size，比如，x是一组图像的矩阵（BATCH_SIZE=100，大小为28×28），则执行x_image = tf.reshape(x, [-1, 28, 28，1])可以计算a=100×28×28/28/28/1=100。即image的维数为（100，28，28）。 把x_image和权重进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max_pooling，形成第一层卷积层 当池化层为2*2时，输出的图像高和宽会缩减一半，所以第一层网络输出，shape为[batch,14,14,6]，6是因为有6个卷积核，一个卷积核输出一个结果，最后6个结果叠到一起形成最终结果 第二层卷积层 #第2层，卷积层 #卷积核大小依然是5*5，通道数为6，卷积核个数为16 W_conv2 = weight_variable([5,5,6,16]) b_conv2 = weight_variable([16]) #h_pool2即为第二层网络输出，shape为[batch,7,7,16] h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2) 卷积核5*5，通道数因为要与输入的数据的通道数相同，所以这里是6，卷积核数量为16 其他的与第一层大致相同 全连接层 #第3层, 全连接层 #这层是拥有120个神经元的全连接层 #W的第1维size为7*7*16，7*7是h_pool2输出的size，16是第2层输出神经元个数 W_fc1 = weight_variable([7*7*16, 120]) b_fc1 = bias_variable([120]) #计算前需要把第2层的输出reshape成[batch, 7*7*16]的张量 h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) #Dropout层 #为了减少过拟合，在输出层前加入dropout keep_prob = tf.placeholder(&quot;float&quot;) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 全连接层，是一个标准的神经网络层 我们要想把上一个卷积层的输出reshape成一个1维列向量，7716 该层的神经元数是120个，所以权重矩阵为[7716,120] 为了防止过拟合加入了dropout层 输出层(softmax层) #输出层 #最后，添加一个softmax层 #可以理解为另一个全连接层，只不过输出时使用softmax将网络输出值转换成了概率 W_fc2 = weight_variable([120, 10]) b_fc2 = bias_variable([10]) y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 因为手写数字有10个，所以最后的权重是[120,10] 定义损失函数和梯度下降 cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv)) #train op, 使用ADAM优化器来做梯度下降。学习率为0.0001 train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) #评估模型，tf.argmax能给出某个tensor对象在某一维上数据最大值的索引。 #因为标签是由0,1组成了one-hot vector，返回的索引就是数值为1的位置 correct_predict = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1)) #计算正确预测项的比例，因为tf.equal返回的是布尔值， #使用tf.cast把布尔值转换成浮点数，然后用tf.reduce_mean求平均值 accuracy = tf.reduce_mean(tf.cast(correct_predict, &quot;float&quot;)) 使用交叉熵来定义损失函数 计算算法的准确率 训练和预测 saver = tf.train.Saver() #开始训练模型，循环20000次，每次随机从训练集中抓取50幅图像 def cnn_train(): # 创建一个交互式Session sess = tf.InteractiveSession() sess.run(tf.initialize_all_variables()) for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: #每100次输出一次日志 train_accuracy = accuracy.eval(feed_dict={ x:batch[0], y_:batch[1], keep_prob:1.0}) print (&quot;step %d, training accuracy %g&quot; % (i, train_accuracy)) saver.save(sess, &apos;./model&apos;) train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob:0.5}) #预测 def predict(): sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) saver = tf.train.Saver(tf.global_variables()) saver.restore(sess, &apos;model&apos;) print( &quot;test accuracy %g&quot; % accuracy.eval(feed_dict={ x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0})) 这里创建了一个Saver类来保存训练好的模型 创建了一个开放的会话，InteractiveSession来保证在运行图的时候，可以随时插入一些计算图，Session要求必须在会话构建之前定义好全部的操作 使用 cnn_train() predict() 由于我在运行时保存了模型，所以在运行时可以吧cnn_train()注释掉，直接运行，可以省略训练过程，如果想查看训练过程可以直接运行","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"}]},{"title":"连读规则","slug":"EnglishPronunciation","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:52.682Z","comments":true,"path":"2019/12/23/EnglishPronunciation/","link":"","permalink":"http://yoursite.com/2019/12/23/EnglishPronunciation/","excerpt":"","text":"Pattern 1 Unvoiced Consonant -&gt; Voiced Consonant 一个词中间的TT要发DD的音，如果TT前边是元音的话，还是TT 例如： DD的情况：butter(bu + der),better(be + der),mettle(me + dle) 不变的情况：mattel,attest,attack 当前边是一个浊辅音节，或者是用es变复数的时候，s要发z的音，以ts结尾的单词都是清辅音 浊辅音：有：[b] [d] [g] [v] [z] [D] [dr] [dz]，此外还有鼻音[m] [n] [ŋ] ，舌则音 [l] [r] 半元音 [w] [j][m] [n] [ŋ] ，这些也都属于浊辅音。 清辅音有：[p]、[t]、[k]、[f]、[θ]、[s]、[W]、[t∫]、[ts]、[tr]，[r]、[h]，合计12个。 例如： rides(rid + z),ladies(ladi + z),bugs(bug + z),angels(angel + z) hats,hates 在一个单词的中间，T要发成齿龈闪音（我感觉是d的音，慢慢体会一哈） 例如： gated(ga + ded),laterel(la + derel),notable(no + dable),water(wa + der) of要发成ov或者uh 例如： game of thrones bottle of water cup of tea Pattern 2 Consonant + Vowel 第一个单词以辅音结尾，第二个单词以元音结尾，一般会把两个发音融合起来 例如： Can I have this? -&gt; Ca nI have this? And I think you’re right. -&gt; An dI think you’ (r)ight How’s it going? -&gt; How (z)it going? Can a cat climb trees -&gt; Ca na cat climb tree(z) Pattern 3 Consonant + Consonant 两个辅音相同的情况下，把音稍微的拖长（包括:S,L,F,M,N,D,J,W,H,V,Z） 例如： This Saturday until later half finished 当两个辅音相同的情况下，在两个音中间做非常短的一个停顿（包括:T,P,K,B,G,C） 例如： Wet towel Big game Gag gift Black cat Pattern 4 Linking “the”(连读the) the + consonant：如果后边的是辅音，the发的发音为the(倒过来的e) 例如： The dog,The cat,The woman the + vowel(长音e除外)：可以发成the(倒过来的e)或者the(ee) 例如： the apple the + 长音e：发the(ee) 例如： the election the evil the eagle Pattern 5 Vowel + Vowel 当元音ee,ih,ay,aye,oi后面紧接的单词是元音的时候，为了因为过度更加自然加入y这个音 例如： He is happy = He(y)iz happy She ate a burger = She(y)ate a burger That boy is hungry = That bo(y)is hungry I ate lunch = I (y)ate lunch Ooh,oh,ow + Vowel：中间插入w 例如： Go in = Go(w)in Do it = Do(w)it You are = You(w)are Go out =Go(w)out How are you = Ho(w)are you Pattern 6 Deletion H-省略 例如： Did he get it = Di diy ge dit he -&gt; iy him -&gt; im him -&gt; iz her -&gt; (倒过来的e)r 无论什么时候，当t或者d在两个辅音之间时都要省略 例如： Old man = Ol man Gold ring = Gol ring Most famous = Mos’ famous Hand bag = Han’ bag Next day = nex day Pattern 7 Transformation(assimilation) 当t后边是j的时候，要发tj(炸 - -|音译来的)的音， 例如： what do you want = watchu want 当d后边是j的时候，要发dj(炸。。。)，貌似就是进行浊化 例如： would you = wouldju 原视频地址","categories":[{"name":"英语","slug":"英语","permalink":"http://yoursite.com/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"连读发音","slug":"连读发音","permalink":"http://yoursite.com/tags/%E8%BF%9E%E8%AF%BB%E5%8F%91%E9%9F%B3/"}]},{"title":"GAN网络代码","slug":"GAN","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:25:44.784Z","comments":true,"path":"2019/12/23/GAN/","link":"","permalink":"http://yoursite.com/2019/12/23/GAN/","excerpt":"","text":"导入头文件 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import os 该函数将给出权重初始化的方法 def variable_init(size): in_dim = size[0] #计算随机生成变量所服从的正态分布标准差 w_stddev = 1. / tf.sqrt(in_dim / 2.) return tf.random_normal(shape=size, stddev=w_stddev) 定义一个可以生成m*n阶随机矩阵的函数，该矩阵的元素服从均匀分布，随机生成的z就为生成器的输入 def sample_Z(m, n): return np.random.uniform(-1., 1., size=[m, n]) 该函数用于输出生成图片 def plot(samples): fig = plt.figure(figsize=(4, 4)) gs = gridspec.GridSpec(4, 4) gs.update(wspace=0.05, hspace=0.05) for i, sample in enumerate(samples): ax = plt.subplot(gs[i]) plt.axis(&apos;off&apos;) ax.set_xticklabels([]) ax.set_yticklabels([]) ax.set_aspect(&apos;equal&apos;) plt.imshow(sample.reshape(28, 28), cmap=&apos;Greys_r&apos;) return fig 定义判别器的权重 #定义输入矩阵的占位符，输入层单元为784，None代表批量大小的占位，X代表输入的真实图片。占位符的数值类型为32位浮点型 X = tf.placeholder(tf.float32, shape=[None, 784]) #定义判别器的权重矩阵和偏置项向量，由此可知判别网络为三层全连接网络 D_W1 = tf.Variable(variable_init([784, 128])) D_b1 = tf.Variable(tf.zeros(shape=[128])) D_W2 = tf.Variable(variable_init([128, 1])) D_b2 = tf.Variable(tf.zeros(shape=[1])) theta_D = [D_W1, D_W2, D_b1, D_b2] 定义生成器的权重 #定义生成器的输入噪声为100维度的向量组，None根据批量大小确定 Z = tf.placeholder(tf.float32, shape=[None, 100]) #定义生成器的权重与偏置项。输入层为100个神经元且接受随机噪声， #输出层为784个神经元，并输出手写字体图片。生成网络根据原论文为三层全连接网络 G_W1 = tf.Variable(variable_init([100, 128])) G_b1 = tf.Variable(tf.zeros(shape=[128])) G_W2 = tf.Variable(variable_init([128, 784])) G_b2 = tf.Variable(tf.zeros(shape=[784])) theta_G = [G_W1, G_W2, G_b1, G_b2] 定义生成器 def generator(z): # 第一层先计算 y=z*G_W1+G-b1,然后投入激活函数计算G_h1=ReLU（y）,G_h1 为第二次层神经网络的输出激活值 G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1) # 以下两个语句计算第二层传播到第三层的激活结果，第三层的激活结果是含有784个元素的向量，该向量转化28×28就可以表示图像 G_log_prob = tf.matmul(G_h1, G_W2) + G_b2 G_prob = tf.nn.sigmoid(G_log_prob) return G_prob 定义判别器 def discriminator(x): # 计算D_h1=ReLU（x*D_W1+D_b1）,该层的输入为含784个元素的向量 D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) # 计算第三层的输出结果。因为使用的是Sigmoid函数，则该输出结果是一个取值为[0,1]间的标量（见上述权重定义） # 即判别输入的图像到底是真（=1）还是假（=0） D_logit = tf.matmul(D_h1, D_W2) + D_b2 D_prob = tf.nn.sigmoid(D_logit) # 返回判别为真的概率和第三层的输入值，输出D_logit是为了将其输入tf.nn.sigmoid_cross_entropy_with_logits()以构建损失函数 return D_prob, D_logit 损失函数 #输入随机噪声z而输出生成样本 G_sample = generator(Z) #分别输入真实图片和生成的图片，并投入判别器以判断真伪 D_real, D_logit_real = discriminator(X) D_fake, D_logit_fake = discriminator(G_sample) # 这里使用交叉熵作为判别器和生成器的损失函数，因为sigmoid_cross_entropy_with_logits内部会对预测输入执行Sigmoid函数， #所以这里取判别器最后一层未投入激活函数的值，即D_h1*D_W2+D_b2。 #tf.ones_like(D_logit_real)创建维度和D_logit_real相等的全是1的标注，真实图片。 D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real))) D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake))) #损失函数为两部分，即E[log(D(x))]+E[log(1-D(G(z)))]，将真的判别为假和将假的判别为真 D_loss = D_loss_real + D_loss_fake #同样使用交叉熵构建生成器损失函数 G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) 优化 #定义判别器和生成器的优化方法为Adam算法，关键字var_list表明最小化损失函数所更新的权重矩阵 D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D) G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G) 设置参数和读取数据 #选择训练的批量大小和随机生成噪声的维度 mb_size = 128 Z_dim = 100 #读取数据集MNIST，并放在当前目录data文件夹下MNIST文件夹中，如果该地址没有数据，则下载数据至该文件夹 mnist = input_data.read_data_sets(&quot;./MNIST_data/&quot;, one_hot=True) 训练过程及结果显示 # 打开一个会话运行计算图 sess = tf.Session() # 初始化所有定义的变量 sess.run(tf.global_variables_initializer()) # 如果当前目录下不存在out文件夹，则创建该文件夹 if not os.path.exists(&apos;out/&apos;): os.makedirs(&apos;out/&apos;) # 初始化，并开始迭代训练，2w次 i = 0 for it in range(20000): # 每2000次输出一张生成器生成的图片 if it % 2000 == 0: samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)}) fig = plot(samples) plt.savefig(&apos;out/{}.png&apos;.format(str(i).zfill(3)), bbox_inches=&apos;tight&apos;) i += 1 plt.close(fig) # next_batch抽取下一个批量的图片，该方法返回一个矩阵，即shape=[mb_size，784]，每一行是一张图片，共批量大小行 X_mb, _ = mnist.train.next_batch(mb_size) # 投入数据并根据优化方法迭代一次，计算损失后返回损失值 _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)}) _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)}) # 每迭代2000次输出迭代数、生成器损失和判别器损失 if it % 2000 == 0: print(&apos;Iter: {}&apos;.format(it)) print(&apos;D loss: {:.4}&apos;.format(D_loss_curr)) print(&apos;G_loss: {:.4}&apos;.format(G_loss_curr)) print()","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"}]},{"title":"Git教程","slug":"Git","date":"2019-12-22T16:12:21.000Z","updated":"2020-02-01T08:38:35.289Z","comments":true,"path":"2019/12/23/Git/","link":"","permalink":"http://yoursite.com/2019/12/23/Git/","excerpt":"","text":"学习廖雪峰Git教程笔记 1. 创建版本库 git init进入文件夹之后再输入指令，将文件创建为一个git管理的文件夹 cd f cd learnGit git init 显示：Initialized empty Git repository git add添加一个文件 readme.txt git add readme.txt git commit git commit -m &quot;wrote a readme file&quot; &quot;-m&quot;是对本次操作的注释 git push git push 执行完add和commit之后，push到远程仓库当中 2. 版本管理 git status查看版本库的当前状态，是否有文件进行改动 $ git status # On branch master # Changes not staged for commit: # (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) # (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) # # modified: readme.txt # no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) git diff显示文件修改的具体内容 $ git diff readme.txt diff --git a/readme.txt b/readme.txt index 46d49bf..9247db6 100644 --- a/readme.txt +++ b/readme.txt @@ -1,2 +1,2 @@ -Git is a version control system. +Git is a distributed version control system. Git is free software. git log显示最近的提交日志，提交的详细信息 $ git log commit 3628164fb26d48395383f8f31179f24e0882e1e0 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 15:11:49 2013 +0800 append GPL commit ea34578d5496d7dd233c827ed32a8cd576c5ee85 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file git reset –hard可以通过git log获得版本信息，HEAD表示当前版本，HEAD^表示上一个版本，HEAD^^表示上上版本，HEAD~100表示前100个版本，也可以在hard后边直接加上版本号，返回对应版本，但是返回之后，git log中返回版本的后边的所有记录会消失，例如：返回到 add distributed版本，那么append GPL就会消失。 当前处于 append GPL版本 $ git reset --hard HEAD^ HEAD is now at ea34578 add distributed 对应的git log $ git log commit ea34578d5496d7dd233c827ed32a8cd576c5ee85 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file 即使后边在log中消失，也可以使用版本号复原，它只是把指针移到了distributed版本 ，但是并没有删除，只是看不到了，所以通过版本号还是可以把指针移回去 $ git reset --hard 3628164 HEAD is now at 3628164 append GPL git reflog此命令是用来记录每一次命令，即使退回到某版本，也可以找到后边的版本号，可以确定回到未来哪个版本 $ git reflog ea34578 HEAD@{0}: reset: moving to HEAD^ 3628164 HEAD@{1}: commit: append GPL ea34578 HEAD@{2}: commit: add distributed cb926e7 HEAD@{3}: commit (initial): wrote a readme file 3. 暂缓区git add之后是把在工作区修改和新增的文件加入到暂存区，git commit是把暂存区的文件添加到版本库当中，接下来考虑一种情况： 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git commit那么这次commit的内容为第一次修改的内容，因为第一次修改通过git add加入到了暂存区当中，而第二次修改没有，而commit只是把暂存区中的内容加入到版本库中，所以这次的内容为第一次修改的内容，如果修改成为下列顺序则没有问题，因为相当于合并两次添加 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git add -&gt; git commit4. 撤回修改 git checkout – file如果还没有加入到缓存区的时候用，可以通过git status来查看对应指令 $ git checkout -- readme.txt命令git checkout – readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况：一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 git reset HEAD file加入到缓存区后想要撤销的时候用，可以通过git satus来查看对应的指令 $ git reset HEAD readme.txt Unstaged changes after reset: M readme.txt 如果加入到版本库中想要撤回只需返回上一个版本即可，对应命令去上边找* 如果已经加入到远程库当中，那就GG* 5. 删除文件 rm file $ rm test.txt删除之后有两种情况 确实要删除此文件，那么就问git rm file删掉，然后commit $ git rm test.txt rm &apos;test.txt&apos; $ git commit -m &quot;remove test.txt&quot; [master d17efd8] remove test.txt 1 file changed, 1 deletion(-) delete mode 100644 test.txt 删错了，那么撤回，和上节一样git checkout – file $ git checkout -- test.txt 注意：命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 6. 远程仓库 注册GitHub账号 如果在主目录下没有.ssh文件夹，则用Git Bash创建，创建过程中一路回车即可，不需设密码，当然如果你想也可以，id_rsa是私钥，不能泄露，id_rsa.pub是公钥，用来添加到GitHub当中，可以放心使用，如果到其他地方办公，只需把办公地的公钥添加到GitHub中即可 $ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 登录后，打开setting，找到Add SSH，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容 本地库和远程库建立连接，现在GitHub中建立一个新的repository 根据提示将本地库的内容添加到远程库中 $ git remote add origin git@github.com:michaelliao/learngit.git 请千万注意，把上面的michaelliao替换成你自己的GitHub账户名 关联后，使用如下命令第一次推送master分支的所有内容，由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。推送成功后，可以立刻在GitHub页面中看到远程库的内容已经和本地一模一样 git push -u origin master此后，每次本地提交后，只要有必要，就可以使用一下命令推送最新修改 git push origin master git clone $ git clone git@github.com:michaelliao/gitskills.git Cloning into &apos;gitskills&apos;... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0) Receiving objects: 100% (3/3), done.你也许还注意到，GitHub给出的地址不止一个，还可以用https://github.com/michaelliao/gitskills.git这样的地址。实际上，Git支持多种协议，默认的git://使用ssh，但也可以使用https等其他协议。使用https除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用ssh协议而只能用https。&nbsp; 要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 7. 创建分支 git checkout -b dev这个指令为创建一个名为dev的分支，并切换到dev分支，相当于git branch dev,git checkout dev两个合二为一 git branch查看当前所有分支，*为当前使用的分支 git merge合并指定分支到当前分支，默认使用Fast forword模式例： 切换到master分支，输入： $ git merge dev 就是把dev合并到master分支上 git branch -d dev删除指定分支 解决冲突要去本地库中手动解决冲突，然后再进行合并，用git log –graph命令可以看到分支合并图。 其他合并模式通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 $ git merge --no-ff -m &quot;merge with no-ff&quot; dev Merge made by the &apos;recursive&apos; strategy. readme.txt | 1 + 1 file changed, 1 insertion(+)因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。合并后，我们用git log看看分支历史：$ git log –graph –pretty=oneline –abbrev-commit + 7825a50 merge with no-ff |\\ | * 6224937 add merge |/ + 59bc1cb conflict fixed ... BUG分支软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是当前正在dev上进行的工作还没有提交，利用 $ git stash Saved working directory and index state WIP on dev: 6224937 add merge HEAD is now at 6224937 add merge现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： $ git stash list stash@{0}: WIP on dev: 6224937 add merge一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除；另一种方式是用git stash pop，恢复的同时把stash内容也删了 多人协作当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认名称是origin。要查看远程库的信息，用git remote： $ git remote origin或者，用git remote -v显示更详细的信息： $ git remote -v origin git@github.com:michaelliao/learngit.git (fetch) origin git@github.com:michaelliao/learngit.git (push)上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。","categories":[{"name":"常用技术","slug":"常用技术","permalink":"http://yoursite.com/categories/%E5%B8%B8%E7%94%A8%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"协作开发软件","slug":"协作开发软件","permalink":"http://yoursite.com/tags/%E5%8D%8F%E4%BD%9C%E5%BC%80%E5%8F%91%E8%BD%AF%E4%BB%B6/"}]},{"title":"JavaScript 高级程序设计","slug":"JavaScript高级程序设计","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:35:09.590Z","comments":true,"path":"2019/12/23/JavaScript高级程序设计/","link":"","permalink":"http://yoursite.com/2019/12/23/JavaScript%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"注意： TypeError: Cannot read property ‘0’ of undefined：很有可能是数组越界 switch在涉及逻辑判断时，速度会下降特别多，所以说不如直接多写几个case。例： for(let i = 1; i&lt; s.length; i ++){ console.log(myArr) switch(true){ case s[i] == &quot;(&quot; || s[i] == &quot;[&quot; || s[i] == &quot;{&quot;: myArr.push(s[i]); break; case s[i] == &quot;)&quot;: if(myArr[myArr.length - 1] == &quot;(&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; case s[i] == &quot;]&quot;: if(myArr[myArr.length - 1] == &quot;[&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; case s[i] == &quot;}&quot;: if(myArr[myArr.length - 1] == &quot;{&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; } } for(let i = 1; i&lt; s.length; i ++){ switch(s[i]){ case &quot;(&quot;: myArr.push(s[i]); break; case &quot;[&quot;: myArr.push(s[i]); break; case &quot;{&quot;: myArr.push(s[i]); break; case &quot;)&quot;: if(myArr[myArr.length - 1] == &quot;(&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; case &quot;]&quot;: if(myArr[myArr.length - 1] == &quot;[&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; case &quot;}&quot;: if(myArr[myArr.length - 1] == &quot;{&quot;){ myArr.pop() }else{ myArr.push(s[i]) } break; } } 这两个代码，速度差7倍！ 第二章：在HTML中使用JavaScript第二章：在HTML中使用JavaScript*","categories":[{"name":"前端技术","slug":"前端技术","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}]},{"title":"LSTM简介及代码","slug":"LSTM","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:46.380Z","comments":true,"path":"2019/12/23/LSTM/","link":"","permalink":"http://yoursite.com/2019/12/23/LSTM/","excerpt":"","text":"1. 简介 作用：为了解决RNN存在的梯度消失问题，即无法实现对某些信息实现长期保存 原理：简单来说就是给RNN加上了一些记忆控制器，使用Forget Gate,Input Gate,Output Gate来控制信息的流动程度。如图： ![][Specify] 上一个LSTM单元得到一个信息a&lt;t-1&gt;，与这个单元的输入x&lt;t&gt;，通过forget门，使用sigmoid函数得到一个值 然后通过update门，使用sigmoid函数得到一个值，再通过tanh函数的得到一个值 三个值，使用如图公式来决定c&lt;t-1&gt;（也就是上一个单元保存的信息）是否需要更新，来得到c&lt;t&gt; 最后通过output门使用tanh函数来得到a&lt;t&gt;来给下一个单元提供信息 2. 代码简介 参数介绍 BATCH_SIZE = 100 # BATCH的大小，相当于一次处理100个image TIME_STEP = 28 # 一个LSTM中，输入序列的长度，image有28行 INPUT_SIZE = 28 # x_i 的向量长度，image有28列 列表中每一个元素都分别对应网络展开的时间步。比如在 MNIST 数据集中，我们有 28x28 像素的图像，每一张都可以看成拥有 28 行 28 个像素的图像。我们将网络按 28 个时间步展开，以使在每一个时间步中，可以输入一行28个像素（input_size），从而经过 28 个时间步输入整张图像。给定图像的 batch_size 值，则每一个时间步将分别收到 batch_size 个图像。如图： ![][time_step] NUM_UNITS = 100 # 多少个LSTM单元 一个LSTM中有多少个LSTM单元。如图： ![][num_units] image = tf.reshape(train_x, [-1, TIME_STEP, INPUT_SIZE]) # 输入的是二维数据，将其还原为三维，维度是[BATCH_SIZE, TIME_STEP, INPUT_SIZE] 这里是将一组图像矩阵train_x重建为新的矩阵，该新矩阵的维数为（a，28，28），其中-1表示a由实际情况来定。比如，x是一组图像的矩阵（BATCH_SIZE=100，大小为28×28），则执行x_image = tf.reshape(x, [-1, 28, 28])可以计算a=100×28×28/28/28=100。即image的维数为（100，28，28）。 rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_UNITS) 定义一个LSTM的LSTM单元数(num_units) outputs,final_state = tf.nn.dynamic_rnn( cell=rnn_cell, # 选择传入的cell inputs=image, # 传入的数据 initial_state=None, # 初始状态 dtype=tf.float32, # 数据类型 time_major=False, # False: (batch, time step, input); True: (time step, batch, input)，这里根据image结构选择False ) output = tf.layers.dense(inputs=outputs[:, -1, :], units=N_CLASSES) 这里outputs,final_state = tf.nn.dynamic_rnn（…）.final_state包含两个量： 第一个为c保存了每个LSTM任务最后一个cell中每个神经元的状态值，状态值是用来确定前边保存的值是否需要更新。 第二个量h保存了每个LSTM任务最后一个cell中每个神经元的输出值，所以c和h的维度都是[BATCH_SIZE,NUM_UNITS]。 outputs的维度是[BATCH_SIZE,TIME_STEP,NUM_UNITS],保存了每个step中cell的输出值h。由于这里是一个many to one的任务，只需要最后一个step的输出outputs[:, -1, :]，output = tf.layers.dense(inputs=outputs[:, -1, :], units=N_CLASSES) 通过一个全连接层将输出限制为N_CLASSES。 loss = tf.losses.softmax_cross_entropy(onehot_labels=train_y, logits=output) # 计算loss train_op = tf.train.AdamOptimizer(LR).minimize(loss) #选择优化方法 correct_prediction = tf.equal(tf.argmax(train_y, axis=1),tf.argmax(output, axis=1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction,’float’)) #计算正确率 定义损失函数，优化算法和计算准确率的方法 sess = tf.Session() sess.run(tf.global_variables_initializer()) # 初始化计算图中的变量 for step in range(ITERATIONS): # 开始训练 x, y = mnist.train.next_batch(BATCH_SIZE) test_x, test_y = mnist.test.next_batch(5000) _, loss_ = sess.run([train_op, loss], {train_x: x, train_y: y}) if step % 500 == 0: # test（validation） accuracy_ = sess.run(accuracy, {train_x: test_x, train_y: test_y}) print(&apos;train loss: %.4f&apos; % loss_, &apos;| test accuracy: %.2f&apos; % accuracy_)这里就是TensorFlow常规的运行规则： 定义一个Session，并初始化上边定义的变量 每次每次获取BATCH_SIZE个图像 对模型进行训练，得到loss 打印准确率","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"}]},{"title":"汉娜·布伦雪尔：给陌生人的情书","slug":"Ted_给陌生人的情书","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:31:01.368Z","comments":true,"path":"2019/12/23/Ted_给陌生人的情书/","link":"","permalink":"http://yoursite.com/2019/12/23/Ted_%E7%BB%99%E9%99%8C%E7%94%9F%E4%BA%BA%E7%9A%84%E6%83%85%E4%B9%A6/","excerpt":"","text":"1. 原版I was one of the only kids in college who had a reason to go the P.O. box at the end of the day,and that was mainly because my mother has never believed in email,in Facebook.in texting or cell phones in general.And so while other kids were BBM-ing their parents I was literally waiting by the mailbox to get a letter from home to see how the weekend had gone,which was a little frustrating when Grandma was in the hospital,but I just looking for some sort of scribble,some unkempt cursive from my mother. So I move to New York city after college and got completely sucker-punched in the face by depression,I did the only thing I could think of at the time.I wrote those same kinds of letters that my mother had written me for strangers,and tucked them all throughout the city, dozens and dozens of them.I left them everywhere,in cafes and in libraries,at the U.N.,everywhere.I blogged about those letters and the days when they were necessary,and I posed a kind of crazy promise to the Internet:that if you asked me for a hand-written letter,I would written you one,no questions asked.Overnight,my in box morphed into this harbor of heartbreak – a single mother in Sacramento,a girl being bullied in rural Kansas,all asking me,a 22-year-old girl,who barely even knew her own coffee order,to write them a love letter and give them a reason to wait by the mailbox. Well,today I fuel a global organization that is fueled by those trips to the mailbox,fueled by the ways in which we can harness social media like never before to write and mail strangers letters when they need them most,but most of all,fueled by crates of mail like this one,my trusty mail crate,filled with the scripting of ordinary people,strangers writing letters to other strangers not because they’re ever going to meet and laugh over a cup of coffee,but because they have found one another by way of letter-writing.But,you know,the thing that always gets me about these letters is that most of them have been wirten by people that have never known themselves loved on a piece of paper.They could not tell you about the ink of their own love letters.They’re the ones from my generation,the ones of us that have grown up into a world where everything is paperless,and where some of our best coversations have happened upon a screen.We have learned to diary our pain onto Facebook,and we speak swiftly in 140 characters or less.But what if its not about effiency this time?I was on the subway yesterday with this mail crate,which is a coversation starter,let me tell you.If you ever need one,just carry one of these.And a man just stared at me,and he was like,”Well,why dont you use the Internet?”And I thought,”Well,sir,I am not a strategist,nor am I specialist.I am merely a storyteller.”And so I could tell you about a wowen whose husband has just come home from Afghanistan,and she is having a hard time unearthing this thing called conversation,and so she tucks love letters throughout the house as a way to say,”Come back to me.Find me when you can.”Or a girl who decides that she is going to leave love letters arrounds her campus in Dubuque,Iowa,only to find her efforts ripple-effected the next day when she walks out onto the quad and finds love letters hanging from the trees,tucked in the bushes and the benches.Or a man who decides that he is going to take his life,uses the Facebook as a way to say goodbye to friends and family.Well,tonight he sleeps safely with a stack of letters just like this one tucked beneath his pillow,scripted by strangers who were there for him when.These are kinds of stories that convinced me that letter-writing will never again need to flip back her hair and talk about the effiency,because she is an art form now,all the parts of her,the signing,the scripting,the mailing,the doodles in the margins.The mere fact that somebody would even just sit down,pull out a piece of paper and think about someone the whole way through,with an intention thai is so much harder to unearth when the browser is up and the iphone is pinging and we’ve got six conversations rolling in at once,that is a art form that does not fall down to the Goliath of “get faster,”no matter how many social networks we might join.We still cluth close those letters to our chest,to the words that speak louder than loud,when we turn pages into palettes to say the things that we have need to say,the words we have needed to write,to sisters and brothers and even to strangers,for far too long.Thank you. 2. 精简版When I grow up,I move to New York city after college /and got completely sucker-punched in the face by depression/,I did the only thing I could think of at the time.I wrote those same kinds of letters for strangers,and tucked them all throughout the city, dozens and dozens of them.I left them everywhere,in cafes and in libraries,at the U.N.,everywhere.I blogged about those letters and the days when they were necessary,and I posed a kind of crazy promise to the Internet:that if you asked me for a hand-written letter,I would written you one,no questions asked.Overnight,my inbox morphed into this harbor of heartbreak – a single mother in Sacramento,a girl being bullied in rural Kansas,all asking me,a 22-year-old girl,who barely even knew her own coffee order,to write them a love letter and give them a reason to wait by the mailbox. Well,today I fuel a global organization/ that is fueled by those trips to the mailbox,/fueled by the ways in which we can harness social media like never before /to write and mail strangers letters when they need them most,but most of all,fueled by crates of mail. like this one,my trusty mail crate,filled with the scripting of ordinary people,strangers writing letters to other strangers not because they’re ever going to meet and laugh over a cup of coffee,but because they have found one another by way of letter-writing.But,you know,the thing that always gets me about these letters/ is that most of them have been wirten /by people that have never known themselves loved on a piece of paper.They could not tell you about the ink of their own love letters.They’re the ones from my generation,the ones of us that have grown up into a world /where everything is paperless,and where some of our best coversations have happened upon a screen.We have learned to diary our pain onto Facebook,and we speak swiftly in 140 characters or less.But what if its not about effiency this time? I was on the subway yesterday with this mail crate,which is a coversation starter,let me tell you.If you ever need one,just carry one of these.And a man just stared at me,and he was like,”Well,why dont you use the Internet?”And I thought,”Well,sir,I am not a strategist,nor am I specialist.I am merely a storyteller.”And so I could tell you about a women whose husband has just come home from Afghanistan,and she is having a hard time unearthing this thing called conversation,and so she tucks love letters throughout the house/ as a way to say,”Come back to me.Find me when you can.”Or a girl who decides that she is going to leave love letters /arrounds her campus in Dubuque,Iowa,/only to find her efforts ripple-effected the next day when she walks out onto the quad /and finds love letters hanging from the trees,/tucked in the bushes and the benches.Or a man who decides that he is going to take his life,/uses the Facebook as a way /to say goodbye/ to friends and family.Well,tonight he sleeps safely with a stack of letters just like this one /tucked beneath his pillow,scripted by strangers who were there for him when.These are kinds of stories that convinced me that letter-writing will never again need to flip back her hair /and talk about the effiency,because she is an art form now,all the parts of her,the signing,the scripting,the mailing,the doodles in the margins.The mere fact that somebody would even just sit down,pull out a piece of paper and think about someone the whole way through,with an intention that is so much harder to unearth/ when the browser is up and the iphone is pinging and we’ve got six conversations rolling in at once,that is an art form /that does not fall down to the Goliath of “get faster,”no matter how many social networks we might join.We still clutch close these letters to our chest,to the words that speak louder than loud,when we turn pages into palettes to say the things that we have need to say,the words we have needed to write,to sisters and brothers /and even to strangers,for far too long.Thank you.","categories":[{"name":"英语","slug":"英语","permalink":"http://yoursite.com/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"Ted","slug":"Ted","permalink":"http://yoursite.com/tags/Ted/"}]},{"title":"强化学习简介","slug":"ReinforcementLearning","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:22:45.595Z","comments":true,"path":"2019/12/23/ReinforcementLearning/","link":"","permalink":"http://yoursite.com/2019/12/23/ReinforcementLearning/","excerpt":"","text":"规则简介 增强学习需要我们定义奖励函数(reward function)，在我们选择的实验环境中： Agent到达G节点，游戏结束，得到奖励1 Agent到达H节点，游戏结束，得到奖励0 Agent到达F或S节点，游戏继续，得到奖励0 # FrozenLake 问题的规则 SFFF (S: 起始点, 安全) FHFH (F: 冰层, 安全) FFFH (H: 空洞, 跌落危险) HFFG (G: 目的地, 飞盘所在地) 导入包及实验环境 import gym import numpy as np import random import tensorflow as tf import matplotlib.pyplot as plt env = gym.make(&apos;FrozenLake-v0&apos;) # 加载实验环境 tf.reset_default_graph() #重新设置计算图(这句话可以省略，没啥影响) 设置占位符 inputs1 = tf.placeholder(shape=[1,16], dtype=tf.float32) # 建立用于选择行为的网络的前向传播部分 W = tf.Variable(tf.random_uniform([16,4], 0, 0.01)) Qout = tf.matmul(inputs1, W) predict = tf.argmax(Qout, 1) nextQ = tf.placeholder(shape=[1,4], dtype=tf.float32) 上边这五行： inputs1：因为这个问题是一个44的格子，每个格子都有一个状态（也就是S,F,H,G,开头提到了），所以这里需要一个116的张量来保存状态 W：每个格子都有4个选择，也就是上下左右移动，所以是一个16*4的矩阵 Qout：四个方向的Q值，也就是奖励值 predict：我们预测的下一步的移动方向 nextQ：我们移动之后的，可以获得的奖励值 计算损失及优化 # 计算预期Q值和目标Q值的差值平方和（损失值） loss = tf.reduce_sum(tf.square(nextQ - Qout)) trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1) updateModel = trainer.minimize(loss) 参数初始化及超参数定义 # 训练网络 init = tf.initialize_all_variables() #初始化 # 设置超参数 y = .99 e = 0.1 num_episodes = 2000 # 创建episodes中包含当前奖励值和步骤的列表 jList = [] rList = [] y:这里就是文档里提到的gamma值，也就是折扣参数 e:这份代码有一个随机事件，就是偶尔会有一阵风吹过，使agent被吹到并非它选择的区域，e为随机事件的概率 num_episodes:这个是我们要进行多少次游戏，也就是要训练多少次 jList:用来保存步骤 rList:用来保存奖励 进行训练 with tf.Session() as sess: sess.run(init) for i in range(num_episodes): # 初始化环境，得到第一个状态观测值 s = env.reset() rAll = 0 d = False j = 0 # Q网络 while j &lt; 99: j += 1 a, allQ = sess.run([predict, Qout], feed_dict={inputs1:np.identity(16)[s:s+1]}) if np.random.rand(1) &lt; e: #判断是否发生随机事件 a[0] = env.action_space.sample() # 获取新的状态值和奖励值 s1, r, d, _ = env.step(a[0]) #d用来判断游戏是否结束，d为true时，游戏结束 # 通过将新的状态值传入网络获取Q&apos;值 Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]}) #判断新的状态我们能获取的奖励值 # 获取最大的Q值并选定我们的动作 maxQ1 = np.max(Q1) targetQ = allQ targetQ[0, a[0]] = r + y*maxQ1 #根据当前奖励以及未来奖励的和来选定我们的动作 # 用目标Q值和预测Q值训练网络 _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ}) #更新权重 rAll += r s = s1 #更新状态和奖励值 if d == True: # 随着训练的进行不断减小随机行动的可能性 e = 1./((i/50) + 10) break jList.append(j) #添加轨迹 rList.append(rAll) #添加奖励，用于判断是否成功 if i%500 == 0: #每500次显示一次成功的数量，根据比值计算准确率，20000次的话，准确率大概在60%多 print(&quot;Percent of succesful episodes:[%d,%d],%f &quot; % (sum(rList),i,sum(rList) / num_episodes)) print(&quot;Percent of succesful episodes: &quot; + str(sum(rList)/num_episodes)) d:设置d来判断当前游戏是否结束 j:这个99，我估计是只要足够他完成游戏即可，大小可以自己设置 rAll:来保存奖励 这段： a, allQ = sess.run([predict, Qout], feed_dict={inputs1:np.identity(16)[s:s+1]}) 根据Q网络和贪心算法(有随机行动的可能)，来选定当前的动作，以及所有动作的Q值（奖励值） 这段： if np.random.rand(1) &lt; e: a[0] = env.action_space.sample() 上边提到，这份代码包含一个随机事件，在这里进行判断，生成一个随机数，如果小于e，则发生随机事件，使agent被吹到并非它选择的区域 这段： s1, r, d, _ = env.step(a[0]) Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]}) maxQ1 = np.max(Q1) targetQ = allQ targetQ[0, a[0]] = r + y*maxQ1 从上到下： 1. 获取新的状态值和奖励值，d用来判断游戏是否结束，d为true时，游戏结束 2. 通过将新的状态值传入网络获取Q&apos;值 3. 判断新的状态我们能获取的奖励值，获取最大的Q值并选定我们的动作 4. 根据当前奖励以及未来奖励的和来选定我们的动作 这段： _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ}) #更新权重 rAll += r s = s1 #更新状态和奖励值 if d == True: # 随着训练的进行不断减小随机行动的可能性 e = 1./((i/50) + 10) break 更新权重，状态及奖励值","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Q网络","slug":"Q网络","permalink":"http://yoursite.com/tags/Q%E7%BD%91%E7%BB%9C/"}]},{"title":"时空图卷积网络","slug":"STGCN","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:47.626Z","comments":true,"path":"2019/12/23/STGCN/","link":"","permalink":"http://yoursite.com/2019/12/23/STGCN/","excerpt":"","text":"####说明： 这个代码我看来好久，也改了好久，效果都不是很好，最开始的时候跑起来时间很长，买了服务器跑一跑效果也不好，我不知道是它的评判方法有问题，还是我放入的数据有问题，后来改了一下时间问题解决了，但是结果还是不好。这里大体的给老师介绍一下，也为以后师兄去跑代码的时候做一点说明吧，希望有点帮助。底层代码我就不详细介绍了，我把大概的流程以及参数设置介绍一下。 环境介绍 Python3.6以上、TensorFlow、pandas、Numpy、Scipy 模型 超参数设置 class Args(object): def __init__(self, n_route, n_his, n_pred, batch_size, epoch, save, ks, kt, lr, opt, graph, inf_mode): self.n_route = n_route self.n_his = n_his self.n_pred = n_pred self.batch_size = batch_size self.epoch = epoch self.save = save self.ks = ks self.kt = kt self.lr = lr self.opt = opt self.graph = graph self.inf_mode = inf_mode &apos;&apos;&apos; 设置可以使用的命令行，我再chrome里存了用法 这里应该是对超参数的设置 &apos;&apos;&apos; # parser = argparse.ArgumentParser() # parser.add_argument(&apos;--n_route&apos;, type=int, default=103) #选取了228个站点,中等数据为228，大型数据为1026 # parser.add_argument(&apos;--n_his&apos;, type=int, default=48) #使用60分钟作为历史事件窗口，因为默认5分钟为time_slot，所以为12 # parser.add_argument(&apos;--n_pred&apos;, type=int, default=24) #12个观测数据点（M = 12）用于预测接下来的15分钟，30分钟和45分钟（H = 3,6,9）的交通状况。这里可以设置为3或6或9 # parser.add_argument(&apos;--batch_size&apos;, type=int, default=16) # parser.add_argument(&apos;--epoch&apos;, type=int, default=50) # parser.add_argument(&apos;--save&apos;, type=int, default=10) # parser.add_argument(&apos;--ks&apos;, type=int, default=3) #ks是图卷积核的大小，它决定了卷积从中心节点开始的最大半径。一般来说，切比雪夫多项式Tk(x)被用于近似核，作为K−1阶展开的一部分，默认为3 # parser.add_argument(&apos;--kt&apos;, type=int, default=3) #kt为时间卷积核大小，时间卷积对输入元素kt个邻居进行操作，默认为3 # parser.add_argument(&apos;--lr&apos;, type=float, default=1e-3) #下边几项就是学习率，优化函数，以及训练模式 # parser.add_argument(&apos;--opt&apos;, type=str, default=&apos;RMSProp&apos;) # parser.add_argument(&apos;--graph&apos;, type=str, default=&apos;default&apos;) # parser.add_argument(&apos;--inf_mode&apos;, type=str, default=&apos;merge&apos;) # # args = parser.parse_args() # print(f&apos;Training configs: {args}&apos;) args = Args(103, 12, 9, 16, 50, 10, 3, 3, 1e-3, &apos;RMSProp&apos;, &apos;default&apos;, &apos;merge&apos;) 这里它是使用CMD或者Ubuntu的终端来运行的，也就是说要运行 ‘python + 参数’来运行，我把代码改了，把超参数封装成一个类，可以从Pycharm里边直接修改超参数和运行。超参数共12个 n_route：表示图中的顶点数，源代码中是使用检测点来当做图中的顶点的，原代码为228个点，在我的代码中，我把地区作为了图的顶点，师兄给我的数据划分为了103个区，所以我设置为了13，用于生成邻接矩阵和数据生成和处理 n_his：多少个数据被当做为历史数据用于预测，原代码是5分钟划分为一个数据，所以一天24个小时就是24*12=288个数据，原代码n_his为12，就是把过去的1个小时作为历史窗口，我的代码把一天以1个小时作为一个数据来划分为24个数据，以12个小时作为历史窗口，来预测接下来9个小时的，其他的数据我也试过，得到的结果差不多，所以还是用了这个。n_his&gt;=10，因为框架由2个ST-cov组成，每个ST-cov由两个时间卷积层和一个空间卷积层组成，每经过一个时间卷积层，数据会被缩小（Kt-1），Kt是时间卷积层中的一个一维卷积的卷积核大小，下边会详细介绍。为什么是10而不是9解释起来有点麻烦，简单的说就是它的最后的输出层使用的也是一个时间卷积层，而它设置了一个Ko（也就是output的卷积核的大小），用作输出层的卷积核的大小，Ko必须&gt;1，所以为10 n_pred：预测接下来多少时间的数据，原代码为9，我这里也为9 batch_size、epoch、save：数据的大小，共训练多少轮，多少轮保存一次模型。 Ks：空间卷积核，也就是图卷积核的大小，用于切比雪夫多项式，它决定了卷积从中心节点开始的最大半径。 Kt：时间卷积的大小，对输入元素的Kt个邻居进行操作，用于提取时间相关性 opt：使用哪个优化函数 graph和inf_mode：目前没发现具体作用。。。。 st-conv block的channel的设置以及拉普拉斯归一化、切比雪夫逼近 blocks = [[1, 32, 64], [64, 32, 128]] # Load wighted adjacency matrix W if args.graph == &apos;default&apos;: W = weight_matrix(pjoin(&apos;./dataset&apos;, f&apos;dis_{n}.csv&apos;)) else: # load customized graph weight matrix W = weight_matrix(pjoin(&apos;./dataset&apos;, args.graph)) # Calculate graph kernel L = scaled_laplacian(W) #使用拉普拉斯矩阵进行归一化 # Alternative approximation method: 1st approx - first_approx(W, n). Lk = cheb_poly_approx(L, Ks, n) #切比雪夫多项式逼近函数 &apos;&apos;&apos; tf.add_to_collection：把变量放入一个集合，把很多变量变成一个列表 tf.get_collection：从一个结合中取出全部变量，是一个列表 &apos;&apos;&apos; tf.add_to_collection(name=&apos;graph_kernel&apos;, value=tf.cast(tf.constant(Lk), tf.float32)) #tf.cast数据类型转换函数，转换成tf.float32 # Data Preprocessing data_file = f&apos;data_{n}.csv&apos; n_train, n_val, n_test = 40, 10, 10 PeMS = data_gen(pjoin(&apos;./dataset&apos;, data_file), (n_train, n_val, n_test), n, n_his + n_pred) #os.path.join()函数用于路径拼接文件路径 print(f&apos;&gt;&gt; Loading dataset with Mean: {PeMS.mean:.2f}, STD: {PeMS.std:.2f}&apos;) if __name__ == &apos;__main__&apos;: model_train(PeMS, blocks, args) model_test(PeMS, PeMS.get_len(&apos;test&apos;), n_his, n_pred, args.inf_mode) weight_matrix():根据他给的公式对邻接矩阵进行处理 scaled_laplacian():使用拉普拉斯矩阵进行归一化 cheb_poly_approx():切比雪夫多项式逼近函数 data_gen():处理数据 n_train, n_val, n_test:数据一共60天，40天用于训练集，10天用于测试集，10天用于验证集，如果时间间隔不是以1个小时为划分的话，这里也要进行对应的调整 PeMS:这里的这个用于训练和测试 model_train()和model_test():训练和测试 data_gen()介绍 def data_gen(file_path, data_config, n_route, n_frame=21, day_slot=24): n_train, n_val, n_test = data_config # generate training, validation and test data try: data_seq = pd.read_csv(file_path, header=None).values except FileNotFoundError: print(f&apos;ERROR: input file was not found in {file_path}.&apos;) seq_train = seq_gen(n_train, data_seq, 0, n_frame, n_route, day_slot) seq_val = seq_gen(n_val, data_seq, n_train, n_frame, n_route, day_slot) seq_test = seq_gen(n_test, data_seq, n_train + n_val, n_frame, n_route, day_slot) # x_stats: dict, the stats for the train dataset, including the value of mean and standard deviation. x_stats = {&apos;mean&apos;: np.mean(seq_train), &apos;std&apos;: np.std(seq_train)} # 归一化 # x_train, x_val, x_test: np.array, [sample_size, n_frame, n_route, channel_size]. x_train = z_score(seq_train, x_stats[&apos;mean&apos;], x_stats[&apos;std&apos;]) #使用z-score进行归一化操作 x_val = z_score(seq_val, x_stats[&apos;mean&apos;], x_stats[&apos;std&apos;]) x_test = z_score(seq_test, x_stats[&apos;mean&apos;], x_stats[&apos;std&apos;]) x_data = {&apos;train&apos;: x_train, &apos;val&apos;: x_val, &apos;test&apos;: x_test} dataset = Dataset(x_data, x_stats) return dataset 根据对应的划分对数据进行处理，对于1个单独一天的数据，它是24个小时，我们12个小时用于历史数据，9个小时我们来预测，这样的话，1天的数据我们就会划分成24-12-9+1个数据，那么对于40天的训练集的数据总数就是40*（24-12-9+1）也就是160个数据，其他的以此类推 划分完后对数据归一化处理，并保存平均值和方差。 model_train()介绍： def model_train(inputs, blocks, args, sum_path=&apos;./output/tensorboard&apos;): &apos;&apos;&apos; Train the base model. :param inputs: instance of class Dataset, data source for training. :param blocks: list, channel configs of st_conv blocks. :param args: instance of class argparse, args for training. &apos;&apos;&apos; n, n_his, n_pred = args.n_route, args.n_his, args.n_pred Ks, Kt = args.ks, args.kt batch_size, epoch, inf_mode, opt = args.batch_size, args.epoch, args.inf_mode, args.opt # Placeholder for model training x = tf.placeholder(tf.float32, [None, n_his + 1, n, 1], name=&apos;data_input&apos;) keep_prob = tf.placeholder(tf.float32, name=&apos;keep_prob&apos;) # Define model loss train_loss, pred = build_model(x, n_his, Ks, Kt, blocks, keep_prob) tf.summary.scalar(&apos;train_loss&apos;, train_loss) #用于tensorboard copy_loss = tf.add_n(tf.get_collection(&apos;copy_loss&apos;)) #tf.add_n([p1, p2, p3....])函数是实现一个列表的元素的相加。就是输入的对象是一个列表，列表里的元素可以是向量，矩阵 tf.summary.scalar(&apos;copy_loss&apos;, copy_loss) # Learning rate settings global_steps = tf.Variable(0, trainable=False) len_train = inputs.get_len(&apos;train&apos;) if len_train % batch_size == 0: epoch_step = len_train / batch_size else: epoch_step = int(len_train / batch_size) + 1 # Learning rate decay with rate 0.7 every 5 epochs. lr = tf.train.exponential_decay(args.lr, global_steps, decay_steps=5 * epoch_step, decay_rate=0.7, staircase=True) tf.summary.scalar(&apos;learning_rate&apos;, lr) step_op = tf.assign_add(global_steps, 1) #tf.assign_add（）将global_steps加上1，必须要经过参数初始化之后才能使用，书签有具体方法 with tf.control_dependencies([step_op]): if opt == &apos;RMSProp&apos;: train_op = tf.train.RMSPropOptimizer(lr).minimize(train_loss) elif opt == &apos;ADAM&apos;: train_op = tf.train.AdamOptimizer(lr).minimize(train_loss) else: raise ValueError(f&apos;ERROR: optimizer &quot;{opt}&quot; is not defined.&apos;) merged = tf.summary.merge_all() #merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。 with tf.Session() as sess: writer = tf.summary.FileWriter(pjoin(sum_path, &apos;train&apos;), sess.graph) sess.run(tf.global_variables_initializer()) if inf_mode == &apos;sep&apos;: # for inference mode &apos;sep&apos;, the type of step index is int. step_idx = n_pred - 1 tmp_idx = [step_idx] min_val = min_va_val = np.array([4e1, 1e5, 1e5]) elif inf_mode == &apos;merge&apos;: # for inference mode &apos;merge&apos;, the type of step index is np.ndarray. step_idx = tmp_idx = np.arange(3, n_pred + 1, 3) - 1 min_val = min_va_val = np.array([4e1, 1e5, 1e5] * len(step_idx)) else: raise ValueError(f&apos;ERROR: test mode &quot;{inf_mode}&quot; is not defined.&apos;) for i in range(epoch): start_time = time.time() for j, x_batch in enumerate( gen_batch(inputs.get_data(&apos;train&apos;), batch_size, dynamic_batch=True, shuffle=True)): summary, _ = sess.run([merged, train_op], feed_dict={x: x_batch[:, 0:n_his + 1, :, :], keep_prob: 1.0}) writer.add_summary(summary, i * epoch_step + j) # if j % 50 == 0: loss_value = \\ sess.run([train_loss, copy_loss], feed_dict={x: x_batch[:, 0:n_his + 1, :, :], keep_prob: 1.0}) print(f&apos;Epoch {i:2d}, Step {j:3d}: [{loss_value[0]:.3f}, {loss_value[1]:.3f}]&apos;) print(f&apos;Epoch {i:2d} Training Time {time.time() - start_time:.3f}s&apos;) start_time = time.time() print(&apos;step_idx:&apos; + str(step_idx)) min_va_val, min_val = \\ model_inference(sess, pred, inputs, batch_size, n_his, n_pred, step_idx, min_va_val, min_val) for ix in tmp_idx: va, te = min_va_val[ix - 2:ix + 1], min_val[ix - 2:ix + 1] print(f&apos;Time Step {ix + 1}: &apos; f&apos;MAPE {va[0]:7.3%}, {te[0]:7.3%}; &apos; f&apos;MAE {va[1]:4.3f}, {te[1]:4.3f}; &apos; f&apos;RMSE {va[2]:6.3f}, {te[2]:6.3f}.&apos;) print(f&apos;Epoch {i:2d} Inference Time {time.time() - start_time:.3f}s&apos;) if (i + 1) % args.save == 0: model_save(sess, global_steps, &apos;STGCN&apos;) writer.close() print(&apos;Training model finished!&apos;) build_model():构建时空图卷积模型 model_inference():将本轮训练好的模型用于预测，得到与真实值的损失 其他的就是正常的设置占位符，然后给数据划分batch_size、设置好超参数准备进行训练 build_model() def build_model(inputs, n_his, Ks, Kt, blocks, keep_prob): &apos;&apos;&apos; Build the base model. :param inputs: placeholder. :param n_his: int, size of historical records for training. :param Ks: int, kernel size of spatial convolution. :param Kt: int, kernel size of temporal convolution. :param blocks: list, channel configs of st_conv blocks. :param keep_prob: placeholder. &apos;&apos;&apos; x = inputs[:, 0:n_his, :, :] #inputs为（?,n_his + 1,103,1） # Ko&gt;0: kernel size of temporal convolution in the output layer. Ko = n_his #因为数据放入时间卷积层他的大小会减小（ks-1），Ko用来作为一个flag # ST-Block for i, channels in enumerate(blocks): x = st_conv_block(x, Ks, Kt, channels, i, keep_prob, act_func=&apos;GLU&apos;) Ko -= 2 * (Ks - 1) #因为有两个时间卷积层，所以 *2 # Output Layer if Ko &gt; 1: y = output_layer(x, Ko, &apos;output_layer&apos;) else: raise ValueError(f&apos;ERROR: kernel size Ko must be greater than 1, but received &quot;{Ko}&quot;.&apos;) tf.add_to_collection(name=&apos;copy_loss&apos;, value=tf.nn.l2_loss(inputs[:, n_his - 1:n_his, :, :] - inputs[:, n_his:n_his + 1, :, :])) train_loss = tf.nn.l2_loss(y - inputs[:, n_his:n_his + 1, :, :]) single_pred = y[:, 0, :, :] tf.add_to_collection(name=&apos;y_pred&apos;, value=single_pred) return train_loss, single_pred 这里使用到了最开始的blocks参数，对st-conv块来提供输入输出的限制。 st_conv_block():来对时空卷积块进行设置 output_layer():对输出层进行设置 train_loss = tf.nn.l2_loss(y - inputs[:, n_his:n_his + 1, :, :]) 根据得到的预测值与真实值进行比较，得到损失值 st_conv_block()介绍： def st_conv_block(x, Ks, Kt, channels, scope, keep_prob, act_func=&apos;GLU&apos;): &apos;&apos;&apos; Spatio-temporal convolutional block, which contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. :param x: tensor, batch_size, time_step, n_route, c_in]. :param Ks: int, kernel size of spatial convolution. :param Kt: int, kernel size of temporal convolution. :param channels: list, channel configs of a single st_conv block. :param scope: str, variable scope. :param keep_prob: placeholder, prob of dropout. :param act_func: str, activation function. :return: tensor, [batch_size, time_step, n_route, c_out]. &apos;&apos;&apos; c_si, c_t, c_oo = channels with tf.variable_scope(f&apos;stn_block_{scope}_in&apos;): x_s = temporal_conv_layer(x, Kt, c_si, c_t, act_func=act_func) x_t = spatio_conv_layer(x_s, Ks, c_t, c_t) with tf.variable_scope(f&apos;stn_block_{scope}_out&apos;): x_o = temporal_conv_layer(x_t, Kt, c_t, c_oo) x_ln = layer_norm(x_o, f&apos;layer_norm_{scope}&apos;) return tf.nn.dropout(x_ln, keep_prob) 从这里就可以看见时空卷积块的具体搭建，类似于一个三明治 temporal_conv_layer() spatio_conv_layer() temporal_conv_layer() 时间卷积层中使用了残差的概念，用了一半数量的卷积核完成卷积，这样就和 P 的维度一致了，然后直接和 P 相加，然后与 sigmoid 激活后的值进行点对点的相乘。 model_inference() def model_inference(sess, pred, inputs, batch_size, n_his, n_pred, step_idx, min_va_val, min_val): &apos;&apos;&apos; Model inference function. :param sess: tf.Session(). :param pred: placeholder. :param inputs: instance of class Dataset, data source for inference. :param batch_size: int, the size of batch. :param n_his: int, the length of historical records for training. :param n_pred: int, the length of prediction. :param step_idx: int or list, index for prediction slice. :param min_va_val: np.ndarray, metric values on validation set. :param min_val: np.ndarray, metric values on test set. &apos;&apos;&apos; x_val, x_test, x_stats = inputs.get_data(&apos;val&apos;), inputs.get_data(&apos;test&apos;), inputs.get_stats() x_tra = inputs.get_data(&apos;train&apos;) if n_his + n_pred &gt; x_val.shape[1]: raise ValueError(f&apos;ERROR: the value of n_pred &quot;{n_pred}&quot; exceeds the length limit.&apos;) y_val, len_val = multi_pred(sess, pred, x_val, batch_size, n_his, n_pred, step_idx) evl_val = evaluation(x_val[0:len_val, step_idx + n_his, :, :], y_val, x_stats) # chks: indicator that reflects the relationship of values between evl_val and min_va_val. chks = evl_val &lt; min_va_val # update the metric on test set, if model&apos;s performance got improved on the validation. if sum(chks): min_va_val[chks] = evl_val[chks] y_pred, len_pred = multi_pred(sess, pred, x_test, batch_size, n_his, n_pred, step_idx) evl_pred = evaluation(x_test[0:len_pred, step_idx + n_his, :, :], y_pred, x_stats) min_val = evl_pred return min_va_val, min_val 将本轮训练好的模型用于测试集，然后得到MAPE、MAE和RMSE等评判标准 结果 Epoch 49,Step 0[数字1 数字2]:数字1为train_loss训练损失，用来计算真实值与预测值之间的误差 数字2为copy_loss，代码为：copy_loss=tf.nn.l2_loss(inputs[:, n_his - 1:n_his, :, :] - inputs[:, n_his:n_his + 1, :, :]),目前没看懂有啥用。。。 再下边就是各种评判标准了。。。不管怎么调整和修改都很离谱。。。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"GCN","slug":"GCN","permalink":"http://yoursite.com/tags/GCN/"}]},{"title":"沉默的大多数","slug":"沉默的大多数","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:48.887Z","comments":true,"path":"2019/12/23/沉默的大多数/","link":"","permalink":"http://yoursite.com/2019/12/23/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0/","excerpt":"","text":"人人有权争胜负，无人有权论是非。实际上，人只要争得了论是非的权利，他已经不战而胜了。 假设某君思想高尚，我是十分敬佩的，可是如果你因此想把我的的脑子挖出来扔掉，换上他的，我绝不肯。人既然活着，就有权保证他思想的连续性，到死方休。 我当然希望自己变得更加善良，但这种善良应该是我变得更聪明造成的，而不是相反。更何况赫拉克利特早就说过，善与恶为一，正如上坡和下坡时同一条路。不知道何为恶，焉知何为善？ 明辨是非的前提就是发展智力，增广见识。(p15) 倘若对自己做价值判断，还要付出一些代价，对别人做价值判断，那就太简单、太舒服了。（p17) 与其大呼小叫说要去解放它们，让人家苦等，倒不如一声不吭，忽然有一天把他们解放，给他们一个意外惊喜。 很多伟大的学者都有狡猾的一面，比如牛顿提出了三大定理之后，为什么要说上帝是万物运动的第一推动力？显然也是朝上帝买个好，万一他真的存在，死后见了面也好说话。 我国传统文化里有“文死谏”之说，这就是说，中国常常就是花刺子模，这种传统就是号召大家做敬业的心事，拿着屁股和脑袋往君王德刀子板子上撞。很显然，只要不是悲观厌世，谁也不喜欢牺牲自己的脑袋和屁股。所以这种号召也是出于滑头分子之后，变着法说君王有理，这样号召只会起反作用。 对于我国的传统文化、现代文化，只从诚实的一面理解是不够的，还要从狡猾的一面来理解。 面对公众和领导时，大家都是信使（这里提到了很多信使因为传递好消息得到了帝王的奖赏，坏消息就被拉去喂老虎），而且都要耍点滑头：捡好听的说或许不至于，起码都在提防着自己不要讲出难听的来–假如混得不好，就该检讨一下自己的嘴是不是不够甜。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"沉默的大多数","slug":"沉默的大多数","permalink":"http://yoursite.com/tags/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0/"}]},{"title":"算法速查","slug":"算法","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:33:49.062Z","comments":true,"path":"2019/12/23/算法/","link":"","permalink":"http://yoursite.com/2019/12/23/%E7%AE%97%E6%B3%95/","excerpt":"","text":"1. 动态规划与分治法 动态规划算法通常用于求解具有某种最优性质的问题。在这类问题中,可能会有许多可行解。每一个解都对应于一个值,我们希望找到具有最优值的解。 基本思想：动态规划算法与分治法类似,其基本思想也是将待求解问题分解成若干个子问题,先求解子问题,然后从这些子问题的解得到原问题的解。 与分治法区别：与分治法不同的是,适合于用动态规划求解的问题,经分解得到子问题往往不是互相独立的。若用分治法来解这类问题,则分解得到的子问题数目太多,有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案,而在需要时再找出已求得的答案,这样就可以避免大量的重复计算,节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到,只要它被计算过,就将其结果填入表中。这就是动态规划法的基本思路。具体的动态规划算法多种多样,但它们具有相同的填表格式。","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Python教程","slug":"python","date":"2019-12-22T16:12:21.000Z","updated":"2019-12-30T16:48:47.026Z","comments":true,"path":"2019/12/23/python/","link":"","permalink":"http://yoursite.com/2019/12/23/python/","excerpt":"","text":"注意： * 不要使用python的内置函数当参数名Python基础(大小写敏感) ‘&#39; 转义字符 r’’ 整体转义 ‘’’……’’’ 可以表示多行，不需要\\n 例如： &gt;&gt;&gt; print(&apos;&apos;&apos;line1 ... line2 ... line3&apos;&apos;&apos;) line1 line2 line3 and、or、not encode()编码、decode()解码 有些时候，字符串里面的%是一个普通字符怎么办，这个时候就需要转义，用%%来表示一个% 定义的不是tuple，是1这个数！这是因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。所以，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义： 例如： &gt;&gt;&gt; t = (1) &gt;&gt;&gt; t 1 &gt;&gt;&gt; t = (1,) &gt;&gt;&gt; t (1,) 定义默认参数要牢记一点：默认参数必须指向不变对象！Python函数在定义的时候，默认参数L的值就被计算出来了，即[]，因为默认参数L也是一个变量，它指向对象[]，每次调用该函数，如果改变了L的内容，则下次调用时，默认参数的内容就变了，不再是函数定义时的[]了 函数 可变参数 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。(与上边进行对比，收到的是tuple而不是list，所以默认参数不会随着调用而被更改。如果默认参数被更改，那么每次调用函数时，得到的返回参数不一样。) Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去 例如： &gt;&gt;&gt; nums = [1, 2, 3] &gt;&gt;&gt; calc(nums[0], nums[1], nums[2]) 14 &gt;&gt;&gt; nums = [1, 2, 3] &gt;&gt;&gt; calc(*nums) 14 关键字参数 可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： def person(name, age, **kw): print(&apos;name:&apos;, name, &apos;age:&apos;, age, &apos;other:&apos;, kw)函数person除了必选参数name和age外，还接受关键字参数kw。在调用该函数时，可以只传入必选参数： &gt;&gt;&gt; person(&apos;Michael&apos;, 30) name: Michael age: 30 other: {}也可以传入任意个数的关键字参数： &gt;&gt;&gt; person(&apos;Bob&apos;, 35, city=&apos;Beijing&apos;) name: Bob age: 35 other: {&apos;city&apos;: &apos;Beijing&apos;} &gt;&gt;&gt; person(&apos;Adam&apos;, 45, gender=&apos;M&apos;, job=&apos;Engineer&apos;) name: Adam age: 45 other: {&apos;gender&apos;: &apos;M&apos;, &apos;job&apos;: &apos;Engineer&apos;}关键字参数有什么用？它可以扩展函数的功能。比如，在person函数里，我们保证能接收到name和age这两个参数，但是，如果调用者愿意提供更多的参数，我们也能收到。试想你正在做一个用户注册的功能，除了用户名和年龄是必填项外，其他都是可选项，利用关键字参数来定义这个函数就能满足注册的需求。 和可变参数类似，也可以先组装出一个dict，然后，把该dict转换为关键字参数传进去： &gt;&gt;&gt; extra = {&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;} &gt;&gt;&gt; person(&apos;Jack&apos;, 24, city=extra[&apos;city&apos;], job=extra[&apos;job&apos;]) name: Jack age: 24 other: {&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;}当然，上面复杂的调用可以用简化的写法： &gt;&gt;&gt; extra = {&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;} &gt;&gt;&gt; person(&apos;Jack&apos;, 24, **extra) name: Jack age: 24 other: {&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;}**extra表示把extra这个dict的所有key-value用关键字参数传入到函数的**kw参数，kw将获得一个dict，注意kw获得的dict是extra的一份拷贝，对kw的改动不会影响到函数外的extra。 命名关键字参数 如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下： def person(name, age, *, city, job): print(name, age, city, job)和关键字参数*kw不同，命名关键字参数需要一个特殊分隔符，*后面的参数被视为命名关键字参数。 调用方式如下： &gt;&gt;&gt; person(&apos;Jack&apos;, 24, city=&apos;Beijing&apos;, job=&apos;Engineer&apos;) Jack 24 Beijing Engineer如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了： def person(name, age, *args, city, job): print(name, age, args, city, job)命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错： &gt;&gt;&gt; person(&apos;Jack&apos;, 24, &apos;Beijing&apos;, &apos;Engineer&apos;) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; TypeError: person() takes 2 positional arguments but 4 were given由于调用时缺少参数名city和job，Python解释器把这4个参数均视为位置参数，但person()函数仅接受2个位置参数。 命名关键字参数可以有缺省值，从而简化调用： def person(name, age, *, city=&apos;Beijing&apos;, job): print(name, age, city, job)由于命名关键字参数city具有默认值，调用时，可不传入city参数： &gt;&gt;&gt; person(&apos;Jack&apos;, 24, job=&apos;Engineer&apos;) Jack 24 Beijing Engineer使用命名关键字参数时，要特别注意，如果没有可变参数，就必须加一个作为特殊分隔符。如果缺少，Python解释器将无法识别位置参数和命名关键字参数： def person(name, age, city, job): # 缺少 *，city和job被视为位置参数 pass 关于*args与**kw *args是可变参数，args接收的是一个tuple；**kw是关键字参数，kw接收的是一个dict。 列表生成式 &gt;&gt;&gt; [x * x for x in range(1, 11)] [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 高级特性 生成器 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： L = [x * x for x in range(10)]L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]g = (x * x for x in range(10))g &lt;generator object at 0x1022ef630&gt; 定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return ‘done’ 注意：这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator，依次返回数字1，3，5： def odd(): print(&apos;step 1&apos;) yield 1 print(&apos;step 2&apos;) yield(3) print(&apos;step 3&apos;) yield(5)调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值： &gt;&gt;&gt; o = odd() &gt;&gt;&gt; next(o) step 1 1 &gt;&gt;&gt; next(o) step 2 3 &gt;&gt;&gt; next(o) step 3 5 &gt;&gt;&gt; next(o) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; StopIteration 迭代器 Interator 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象 函数式编程 高级函数：可以以函数作为参数的函数 map()函数：接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。举例说明，比如我们有一个函数f(x)=x2，要把这个函数作用在一个list [1, 2, 3, 4, 5, 6, 7, 8, 9]上，就可以用map()实现如下： def f(x): … return x * x …r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] reduce()函数：reduce把一个函数作用在一个序列[x1, x2, x3, …]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 比方说对一个序列求和，就可以用reduce实现： &gt;&gt;&gt; from functools import reduce &gt;&gt;&gt; def add(x, y): ... return x + y ... &gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9]) 25 sorted()函数：也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序： sorted([36, 5, -12, 9, -21], key=abs) [5, 9, -12, -21, 36] key指定的函数将作用于list的每一个元素上，并根据key函数返回的结果进行排序。对比原始的list和经过key=abs处理过的list： list = [36, 5, -12, 9, -21] keys = [36, 5, 12, 9, 21]要进行反向排序，不必改动key函数，可以传入第三个参数reverse=True： &gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True) [&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;] 闭包 def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数： &gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9) &gt;&gt;&gt; f &lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt;注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。 另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。我们来看一个例子： def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fs f1, f2, f3 = count()在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的3个函数都返回了。 你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： &gt;&gt;&gt; f1() 9 &gt;&gt;&gt; f2() 9 &gt;&gt;&gt; f3() 9全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs再看看结果： &gt;&gt;&gt; f1, f2, f3 = count() &gt;&gt;&gt; f1() 1 &gt;&gt;&gt; f2() 4 &gt;&gt;&gt; f3() 9缺点是代码较长，可利用lambda函数缩短代码。 匿名函数 当我们在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。 在Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算f(x)=x2时，除了定义一个f(x)的函数外，还可以直接传入匿名函数： &gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])) [1, 4, 9, 16, 25, 36, 49, 64, 81]通过对比可以看出，匿名函数lambda x: x * x实际上就是： def f(x): return x * x关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： &gt;&gt;&gt; f = lambda x: x * x &gt;&gt;&gt; f &lt;function &lt;lambda&gt; at 0x101c6ef28&gt; &gt;&gt;&gt; f(5) 25同样，也可以把匿名函数作为返回值返回，比如： def build(x, y): return lambda: x * x + y * y 装饰器 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。 &gt;&gt;&gt; def now(): ... print(&apos;2015-3-25&apos;) ... &gt;&gt;&gt; f = now &gt;&gt;&gt; f() 2015-3-25函数对象有一个name属性，可以拿到函数的名字： &gt;&gt;&gt; now.__name__ &apos;now&apos; &gt;&gt;&gt; f.__name__ &apos;now&apos;现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： def log(func): def wrapper(*args, **kw): print(&apos;call %s():&apos; % func.__name__) return func(*args, **kw) return wrapper观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处： @log def now(): print(&apos;2015-3-25&apos;)调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： &gt;&gt;&gt; now() call now(): 2015-3-25把@log放到now()函数的定义处，相当于执行了语句： now = log(now)以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有name等属性，但你去看经过decorator装饰之后的函数，它们的name已经从原来的’now’变成了’wrapper’： &gt;&gt;&gt; now.__name__ &apos;wrapper&apos;因为返回的那个wrapper()函数名字就是’wrapper’，所以，需要把原始函数的name等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.name = func.name这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下： import functools def log(func): @functools.wraps(func) def wrapper(*args, **kw): print(&apos;call %s():&apos; % func.__name__) return func(*args, **kw) return wrapper或者针对带参数的decorator： import functools def log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print(&apos;%s %s():&apos; % (text, func.__name__)) return func(*args, **kw) return wrapper return decoratorimport functools是导入functools模块。模块的概念稍候讲解。现在，只需记住在定义wrapper()的前面加上@functools.wraps(func)即可。 偏函数 functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2： &gt;&gt;&gt; import functools &gt;&gt;&gt; int2 = functools.partial(int, base=2) &gt;&gt;&gt; int2(&apos;1000000&apos;) 64 &gt;&gt;&gt; int2(&apos;1010101&apos;) 85所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 sys模块的argv变量 sys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如： 运行python3 hello.py获得的sys.argv就是[‘hello.py’]； 运行python3 hello.py Michael获得的sys.argv就是[‘hello.py’, ‘Michael]。 面向对象编程 类和示例 例如：仍以Student类为例，在Python中，定义类是通过class关键字： class Student(object): pass 类名通常是大写开头的单词 紧接着是(object)，表示该类是从哪个类继承下来的，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 可以自由地给一个实例变量绑定属性，比如，给实例bart绑定一个name属性（可以随时绑定属性）： &gt;&gt;&gt; bart.name = &apos;Bart Simpson&apos; &gt;&gt;&gt; bart.name &apos;Bart Simpson&apos; 访问限制（私有属性） 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(&apos;%s: %s&apos; % (self.__name, self.__score))改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.name和实例变量.score了： &gt;&gt;&gt; bart = Student(&apos;Bart Simpson&apos;, 59) &gt;&gt;&gt; bart.__name Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AttributeError: &apos;Student&apos; object has no attribute &apos;__name&apos;这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。 注意： 需要注意的是，在Python中，变量名类似xxx的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用name、__score__这样的变量名。 有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问name是因为Python解释器对外把name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量： &gt;&gt;&gt; bart._Student__name &apos;Bart Simpson&apos;但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。 总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。 继承和多态（Python的继承不是特别的严格！） 静态语言 VS 动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了： class Timer(object): def run(self): print(&apos;Start...&apos;)这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 Python的“file-like object“就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为“file-like object“。许多函数接收的参数就是“file-like object“，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 isinstance() 对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。 我们回顾上次的例子，如果继承关系是： object -&gt; Animal -&gt; Dog -&gt; Husky那么，isinstance()就可以告诉我们，一个对象是否是某种类型。先创建3种类型的对象： &gt;&gt;&gt; a = Animal() &gt;&gt;&gt; d = Dog() &gt;&gt;&gt; h = Husky()然后，判断： &gt;&gt;&gt; isinstance(h, Husky) True没有问题，因为h变量指向的就是Husky对象。 再判断： &gt;&gt;&gt; isinstance(h, Dog) Trueh虽然自身是Husky类型，但由于Husky是从Dog继承下来的，所以，h也还是Dog类型。换句话说，isinstance()判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上。 因此，我们可以确信，h还是Animal类型： &gt;&gt;&gt; isinstance(h, Animal) True dir() 函数：可以获得一个对象的所有属性和方法，它返回一个包含字符串的list 类似xxx的属性和方法在Python中都是有特殊用途的，比如len方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的len()方法，所以，下面的代码是等价的： &gt;&gt;&gt; len(&apos;ABC&apos;) 3 &gt;&gt;&gt; &apos;ABC&apos;.__len__() 3我们自己写的类，如果也想用len(myObj)的话，就自己写一个len()方法： &gt;&gt;&gt; class MyDog(object): ... def __len__(self): ... return 100 ... &gt;&gt;&gt; dog = MyDog() &gt;&gt;&gt; len(dog) 100 面对对象高级编程 __slots__ 但是，如果我们想要限制实例的属性怎么办？比如，只允许对Student实例添加name和age属性。 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的slots变量，来限制该class实例能添加的属性： class Student(object): __slots__ = (&apos;name&apos;, &apos;age&apos;) # 用tuple定义允许绑定的属性名称然后，我们试试： &gt;&gt;&gt; s = Student() # 创建新的实例 &gt;&gt;&gt; s.name = &apos;Michael&apos; # 绑定属性&apos;name&apos; &gt;&gt;&gt; s.age = 25 # 绑定属性&apos;age&apos; &gt;&gt;&gt; s.score = 99 # 绑定属性&apos;score&apos; Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AttributeError: &apos;Student&apos; object has no attribute &apos;score&apos;由于’score’没有被放到slots中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。 使用slots要注意，slots定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： &gt;&gt;&gt; class GraduateStudent(Student): ... pass ... &gt;&gt;&gt; g = GraduateStudent() &gt;&gt;&gt; g.score = 9999除非在子类中也定义slots，这样，子类实例允许定义的属性就是自身的slots加上父类的slots。 @property 在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，导致可以把成绩随便改： s = Student() s.score = 9999这显然不合逻辑。为了限制score的范围，可以通过一个set_score()方法来设置成绩，再通过一个get_score()来获取成绩，这样，在set_score()方法里，就可以检查参数： class Student(object): def get_score(self): return self._score def set_score(self, value): if not isinstance(value, int): raise ValueError(&apos;score must be an integer!&apos;) if value &lt; 0 or value &gt; 100: raise ValueError(&apos;score must between 0 ~ 100!&apos;) self._score = value现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： &gt;&gt;&gt; s = Student() &gt;&gt;&gt; s.set_score(60) # ok! &gt;&gt;&gt; s.get_score() 60 &gt;&gt;&gt; s.set_score(9999) Traceback (most recent call last): ... ValueError: score must between 0 ~ 100!但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？对于追求完美的Python程序员来说，这是必须要做到的！ 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的： class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(&apos;score must be an integer!&apos;) if value &lt; 0 or value &gt; 100: raise ValueError(&apos;score must between 0 ~ 100!&apos;) self._score = value@property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： &gt;&gt;&gt; s = Student() &gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60) &gt;&gt;&gt; s.score # OK，实际转化为s.get_score() 60 &gt;&gt;&gt; s.score = 9999 Traceback (most recent call last): ... ValueError: score must between 0 ~ 100!注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。 总结：这个方法就是把方法变成了类，同时通过类名.setter来实现对类名的验证 多重继承 命名规则： MixIn：在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。 为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn： class Dog(Mammal, RunnableMixIn, CarnivorousMixIn): pass 定制类（自己去翻教程！！） 枚举类（自己去翻教程！！） 错误、调试和测试 try、except、finally 如果使用了try、except、finally，不管有没有出错，finally一定会被执行 如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句： try: print(&apos;try...&apos;) r = 10 / int(&apos;2&apos;) print(&apos;result:&apos;, r) except ValueError as e: print(&apos;ValueError:&apos;, e) except ZeroDivisionError as e: print(&apos;ZeroDivisionError:&apos;, e) else: print(&apos;no error!&apos;) finally: print(&apos;finally...&apos;) print(‘END’) 记录错误 如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去。 Python内置的logging模块可以非常容易地记录错误信息： # err_logging.py import logging def foo(s): return 10 / int(s) def bar(s): return foo(s) * 2 def main(): try: bar(&apos;0&apos;) except Exception as e: logging.exception(e) main() print(&apos;END&apos;)同样是出错，但程序打印完错误信息后会继续执行，并正常退出： $ python3 err_logging.py ERROR:root:division by zero Traceback (most recent call last): File &quot;err_logging.py&quot;, line 13, in main bar(&apos;0&apos;) File &quot;err_logging.py&quot;, line 9, in bar return foo(s) * 2 File &quot;err_logging.py&quot;, line 6, in foo return 10 / int(s) ZeroDivisionError: division by zero END通过配置，logging还可以把错误记录到日志文件里，方便事后排查。 调试可以不用print来调试的方法！！！！！！！！ 凡是用print()来辅助查看的地方，都可以用断言（assert）来替代： def foo(s): n = int(s) assert n != 0, &apos;n is zero!&apos; return 10 / n def main(): foo(&apos;0&apos;) assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 如果断言失败，assert语句本身就会抛出AssertionError： $ python err.py Traceback (most recent call last): ... AssertionError: n is zero! 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert： $ python -O err.py Traceback (most recent call last): ... ZeroDivisionError: division by zero 关闭后，你可以把所有的assert语句当成pass来看。io编程 文件读写 文件关闭问题 如果文件打开成功，接下来，调用read()方法可以一次读取文件的全部内容，Python把内容读到内存，用一个str对象表示： &gt;&gt;&gt; f.read() &apos;Hello, world!&apos; 最后一步是调用close()方法关闭文件。文件使用完毕后必须关闭，因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： &gt;&gt;&gt; f.close() &gt;&gt;&gt; 由于文件读写时都有可能产生IOError，一旦出错，后面的f.close()就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用try … finally来实现： try: f = open(&apos;/path/to/file&apos;, &apos;r&apos;) print(f.read()) finally: if f: f.close() 但是每次都这么写实在太繁琐，所以，Python引入了with语句来自动帮我们调用close()方法： with open(&apos;/path/to/file&apos;, &apos;r&apos;) as f: print(f.read()) 这和前面的try … finally是一样的，但是代码更佳简洁，并且不必调用f.close()方法。 读取大小问题 调用read()会一次性读取文件的全部内容，如果文件有10G，内存就爆了，所以，要保险起见，可以反复调用read(size)方法，每次最多读取size个字节的内容。另外，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回list。因此，要根据需要决定怎么调用。 如果文件很小，read()一次性读取最方便；如果不能确定文件大小，反复调用read(size)比较保险；如果是配置文件，调用readlines()最方便： 二进制文件 前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用’rb’模式打开文件即可： &gt;&gt;&gt; f = open(&apos;/Users/michael/test.jpg&apos;, &apos;rb&apos;) &gt;&gt;&gt; f.read() b&apos;\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...&apos; # 十六进制表示的字节 字符编码 要读取非UTF-8编码的文本文件，需要给open()函数传入encoding参数，例如，读取GBK编码的文件： &gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;) &gt;&gt;&gt; f.read() &apos;测试&apos;遇到有些编码不规范的文件，你可能会遇到UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open()函数还接收一个errors参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： &gt;&gt;&gt; f = open(&apos;/Users/michael/gbk.txt&apos;, &apos;r&apos;, encoding=&apos;gbk&apos;, errors=&apos;ignore&apos;) 写文件 要写入特定编码的文本文件，请给open()函数传入encoding参数，将字符串自动转换成指定编码。 细心的童鞋会发现，以’w’模式写入文件时，如果文件已存在，会直接覆盖（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以传入’a’以追加（append）模式写入。 file-like Object（也就是前边在继承那里提到的鸭子类型） 像open()函数返回的这种有个read()方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个read()方法就行。 StringIO就是在内存中创建的file-like Object，常用作临时缓冲。 JSON Python内置的json模块提供了非常完善的Python对象到JSON格式的转换。我们先看看如何把Python对象变成一个JSON： import jsond = dict(name=’Bob’, age=20, score=88)json.dumps(d) ‘{“age”: 20, “score”: 88, “name”: “Bob”}’ dumps()方法返回一个str，内容就是标准的JSON。类似的，dump()方法可以直接把JSON写入一个file-like Object。 要把JSON反序列化为Python对象，用loads()或者对应的load()方法，前者把JSON的字符串反序列化，后者从file-like Object中读取字符串并反序列化： &gt;&gt;&gt; json_str = &apos;{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}&apos; &gt;&gt;&gt; json.loads(json_str) {&apos;age&apos;: 20, &apos;score&apos;: 88, &apos;name&apos;: &apos;Bob&apos;}由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的str与JSON的字符串之间转换。 当我们想保存一个class时，内置的json是不能直接使用的，我们需要参考他的参数，以及自己制定一个转换函数 可选参数就是让我们来定制JSON序列化。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps()方法不知道如何将Student实例变为一个JSON的{}对象。 可选参数default就是把任意一个对象变成一个可序列为JSON的对象，我们只需要为Student专门写一个转换函数，再把函数传进去即可： def student2dict(std): return { &apos;name&apos;: std.name, &apos;age&apos;: std.age, &apos;score&apos;: std.score }这样，Student实例首先被student2dict()函数转换成dict，然后再被顺利序列化为JSON： &gt;&gt;&gt; print(json.dumps(s, default=student2dict)) {&quot;age&quot;: 20, &quot;name&quot;: &quot;Bob&quot;, &quot;score&quot;: 88}不过，下次如果遇到一个Teacher类的实例，照样无法序列化为JSON。我们可以偷个懒，把任意class的实例变为dict： print(json.dumps(s, default=lambda obj: obj.__dict__))因为通常class的实例都有一个dict属性，它就是一个dict，用来存储实例变量。也有少数例外，比如定义了slots的class。 同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法首先转换出一个dict对象，然后，我们传入的object_hook函数负责把dict转换为Student实例： def dict2student(d): return Student(d[&apos;name&apos;], d[&apos;age&apos;], d[&apos;score&apos;])运行结果如下： &gt;&gt;&gt; json_str = &apos;{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}&apos; &gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student)) &lt;__main__.Student object at 0x10cd3c190&gt;打印出的是反序列化的Student实例对象。 进程与线程（没看！！！！）正则表达式 基础知识 字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。 正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 所以我们判断一个字符串是否是合法的Email的方法是： 创建一个匹配Email的正则表达式； 用该正则表达式去匹配用户的输入来判断是否合法。 因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。 在正则表达式中，如果直接给出字符，就是精确匹配。用\\d可以匹配一个数字，\\w可以匹配一个字母或数字，所以： ‘00\\d’可以匹配’007’，但无法匹配’00A’； ‘\\d\\d\\d’可以匹配’010’； ‘\\w\\w\\d’可以匹配’py3’； .可以匹配任意字符，所以： ‘py.’可以匹配’pyc’、’pyo’、’py!’等等。 要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符： 来看一个复杂的例子：\\d{3}\\s+\\d{3,8}。 我们来从左到右解读一下： \\d{3}表示匹配3个数字，例如’010’； \\s可以匹配一个空格（也包括Tab等空白符），所以\\s+表示至少有一个空格，例如匹配’ ‘，’ ‘等； \\d{3,8}表示3-8个数字，例如’1234567’。 综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。 如果要匹配’010-12345’这样的号码呢？由于’-‘是特殊字符，在正则表达式中，要用’&#39;转义，所以，上面的正则是\\d{3}-\\d{3,8}。 但是，仍然无法匹配’010 - 12345’，因为带有空格。所以我们需要更复杂的匹配方式。 进阶 要做更精确地匹配，可以用[]表示范围，比如： [0-9a-zA-Z_]可以匹配一个数字、字母或者下划线； [0-9a-zA-Z_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如’a100’，’0_Z’，’Py3000’等等； [a-zA-Z_][0-9a-zA-Z_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z_][0-9a-zA-Z_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配’Python’或者’python’。 ^表示行的开头，^\\d表示必须以数字开头。 $表示行的结束，\\d$表示必须以数字结束。 你可能注意到了，py也可以匹配’python’，但是加上^py$就变成了整行匹配，就只能匹配’py’了。 Python中使用正则表达式 re模块 有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\转义，所以要特别注意： s = &apos;ABC\\\\-001&apos; # Python的字符串 # 对应的正则表达式字符串变成： # &apos;ABC\\-001&apos;因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了： s = r&apos;ABC\\-001&apos; # Python的字符串 # 对应的正则表达式字符串不变： # &apos;ABC\\-001&apos;先看看如何判断正则表达式是否匹配： &gt;&gt;&gt; import re &gt;&gt;&gt; re.match(r&apos;^\\d{3}\\-\\d{3,8}$&apos;, &apos;010-12345&apos;) &lt;_sre.SRE_Match object; span=(0, 9), match=&apos;010-12345&apos;&gt; &gt;&gt;&gt; re.match(r&apos;^\\d{3}\\-\\d{3,8}$&apos;, &apos;010 12345&apos;) &gt;&gt;&gt;match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是： test = &apos;用户输入的字符串&apos; if re.match(r&apos;正则表达式&apos;, test): print(&apos;ok&apos;) else: print(&apos;failed&apos;) 使用re模块进行字符串切割（如果用户输入了一组标签，用正则表达式来把不规范的输入转化成正确的数组。） 用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码： &gt;&gt;&gt; &apos;a b c&apos;.split(&apos; &apos;) [&apos;a&apos;, &apos;b&apos;, &apos;&apos;, &apos;&apos;, &apos;c&apos;]嗯，无法识别连续的空格，用正则表达式试试： &gt;&gt;&gt; re.split(r&apos;\\s+&apos;, &apos;a b c&apos;) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]无论多少个空格都可以正常分割。加入,试试： &gt;&gt;&gt; re.split(r&apos;[\\s\\,]+&apos;, &apos;a,b, c d&apos;) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]再加入;试试： &gt;&gt;&gt; re.split(r&apos;[\\s\\,\\;]+&apos;, &apos;a,b;; c d&apos;) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;] 提取子串，进行分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： &gt;&gt;&gt; m = re.match(r&apos;^(\\d{3})-(\\d{3,8})$&apos;, &apos;010-12345&apos;) &gt;&gt;&gt; m &lt;_sre.SRE_Match object; span=(0, 9), match=&apos;010-12345&apos;&gt; &gt;&gt;&gt; m.group(0) &apos;010-12345&apos; &gt;&gt;&gt; m.group(1) &apos;010&apos; &gt;&gt;&gt; m.group(2) &apos;12345&apos;如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。 注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。 贪婪匹配 最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： &gt;&gt;&gt; re.match(r&apos;^(\\d+)(0*)$&apos;, &apos;102300&apos;).groups() (&apos;102300&apos;, &apos;&apos;)由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配： &gt;&gt;&gt; re.match(r&apos;^(\\d+?)(0*)$&apos;, &apos;102300&apos;).groups() (&apos;1023&apos;, &apos;00&apos;) 常用内建模块 常用 datetime 时间 collections 集合 双向列表 deque 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。 deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： &gt;&gt;&gt; from collections import deque &gt;&gt;&gt; q = deque([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]) &gt;&gt;&gt; q.append(&apos;x&apos;) &gt;&gt;&gt; q.appendleft(&apos;y&apos;) &gt;&gt;&gt; q deque([&apos;y&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;x&apos;]) deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。 计数器 Counter是一个简单的计数器，例如，统计字符出现的个数： &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; c = Counter() &gt;&gt;&gt; for ch in &apos;programming&apos;: ... c[ch] = c[ch] + 1 ... &gt;&gt;&gt; c Counter({&apos;g&apos;: 2, &apos;m&apos;: 2, &apos;r&apos;: 2, &apos;a&apos;: 1, &apos;i&apos;: 1, &apos;o&apos;: 1, &apos;n&apos;: 1, &apos;p&apos;: 1}) Counter实际上也是dict的一个子类，上面的结果可以看出，字符’g’、’m’、’r’各出现了两次，其他字符各出现了一次。 Base64 是一种用64个字符来表示任意二进制数据的方法。 struct 解决bytes和其他二进制数据类型的转换。 hashlib 提供了常见的摘要算法，如MD5，SHA1等等。（用于密码加密，用到的时候可以详细看一下网址） 什么是摘要算法呢？摘要算法又称哈希算法、散列算法。它通过一个函数，把任意长度的数据转换为一个长度固定的数据串（通常用16进制的字符串表示）。 hmac 与hashlib一起使用 通过哈希算法，我们可以验证一段数据是否有效，方法就是对比该数据的哈希值，例如，判断用户口令是否正确，我们用保存在数据库中的password_md5对比计算md5(password)的结果，如果一致，用户输入的口令就是正确的。 为了防止黑客通过彩虹表根据哈希值反推原始口令，在计算哈希的时候，不能仅针对原始输入计算，需要增加一个salt来使得相同的输入也能得到不同的哈希，这样，大大增加了黑客破解的难度。 如果salt是我们自己随机生成的，通常我们计算MD5时采用md5(message + salt)。但实际上，把salt看做一个“口令”，加salt的哈希就是：计算一段message的哈希时，根据不通口令计算出不同的哈希。要验证哈希值，必须同时提供正确的口令。 这实际上就是Hmac算法：Keyed-Hashing for Message Authentication。它通过一个标准算法，在计算哈希的过程中，把key混入计算过程中。 和我们自定义的加salt算法不同，Hmac算法针对所有哈希算法都通用，无论是MD5还是SHA-1。采用Hmac替代我们自己的salt算法，可以使程序算法更标准化，也更安全。 Python自带的hmac模块实现了标准的Hmac算法。 itertools Python的内建模块itertools提供了非常有用的用于操作迭代对象的函数。 contextlib urllib(爬虫可能偶尔会用) XML HTMLPaeser 连接数据库MySQL 安装MySQL驱动 由于MySQL服务器以独立的进程运行，并通过网络对外服务，所以，需要支持Python的MySQL驱动来连接到MySQL服务器。MySQL官方提供了mysql-connector-python驱动，但是安装的时候需要给pip命令加上参数–allow-external： $ pip install mysql-connector-python --allow-external mysql-connector-python如果上面的命令安装失败，可以试试另一个驱动： $ pip install mysql-connector我们演示如何连接到MySQL服务器的test数据库： # 导入MySQL驱动: &gt;&gt;&gt; import mysql.connector # 注意把password设为你的root口令: &gt;&gt;&gt; conn = mysql.connector.connect(user=&apos;root&apos;, password=&apos;password&apos;, database=&apos;test&apos;) &gt;&gt;&gt; cursor = conn.cursor() # 创建user表: &gt;&gt;&gt; cursor.execute(&apos;create table user (id varchar(20) primary key, name varchar(20))&apos;) # 插入一行记录，注意MySQL的占位符是%s: &gt;&gt;&gt; cursor.execute(&apos;insert into user (id, name) values (%s, %s)&apos;, [&apos;1&apos;, &apos;Michael&apos;]) &gt;&gt;&gt; cursor.rowcount 1 # 提交事务: &gt;&gt;&gt; conn.commit() &gt;&gt;&gt; cursor.close() # 运行查询: &gt;&gt;&gt; cursor = conn.cursor() &gt;&gt;&gt; cursor.execute(&apos;select * from user where id = %s&apos;, (&apos;1&apos;,)) &gt;&gt;&gt; values = cursor.fetchall() &gt;&gt;&gt; values [(&apos;1&apos;, &apos;Michael&apos;)] # 关闭Cursor和Connection: &gt;&gt;&gt; cursor.close() True &gt;&gt;&gt; conn.close() 由于Python的DB-API定义都是通用的，所以，操作MySQL的数据库代码和SQLite类似。 Web开发（内涵MVC简单介绍） CS与BS的区别（今天才知道- -）： CS架构不适合Web，最大的原因是Web应用程序的修改和升级非常迅速，而CS架构需要每个客户端逐个升级桌面App，因此，Browser/Server模式开始流行，简称BS架构。在BS架构下，客户端只需要浏览器，应用程序的逻辑和数据都存储在服务器端。浏览器只需要请求服务器，获取Web页面，并把Web页面展示给用户即可。 WSGI 需要一个统一的接口，让我们专心用Python编写Web业务。这个接口就是WSGI：Web Server Gateway Interface。 def application(environ, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)]) return [b&apos;&lt;h1&gt;Hello, web!&lt;/h1&gt;&apos;]上面的application()函数就是符合WSGI标准的一个HTTP处理函数，它接收两个参数： environ：一个包含所有HTTP请求信息的dict对象； start_response：一个发送HTTP响应的函数。 在application()函数中，调用： start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)])就发送了HTTP响应的Header，注意Header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是HTTP响应码，一个是一组list表示的HTTP Header，每个Header用一个包含两个str的tuple表示。 通常情况下，都应该把Content-Type头发送给浏览器。其他很多常用的HTTP Header也应该发送。 然后，函数的返回值b’Hello, web!‘将作为HTTP响应的Body发送给浏览器。 有了WSGI，我们关心的就是如何从environ这个dict对象拿到HTTP请求信息，然后构造HTML，通过start_response()发送Header，最后返回Body。 整个application()函数本身没有涉及到任何解析HTTP的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。 使用模板（这里介绍了MVC）MVC 异步IO (去翻教程吧，一句两句总结不清楚)","categories":[{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"Flex 布局","slug":"Flex","date":"2019-01-22T16:12:21.000Z","updated":"2020-01-12T13:42:02.007Z","comments":true,"path":"2019/01/23/Flex/","link":"","permalink":"http://yoursite.com/2019/01/23/Flex/","excerpt":"","text":"链接","categories":[{"name":"前端技术","slug":"前端技术","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Flex","slug":"Flex","permalink":"http://yoursite.com/tags/Flex/"}]}],"categories":[{"name":"前端技术","slug":"前端技术","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"英语","slug":"英语","permalink":"http://yoursite.com/categories/%E8%8B%B1%E8%AF%AD/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"常用技术","slug":"常用技术","permalink":"http://yoursite.com/categories/%E5%B8%B8%E7%94%A8%E6%8A%80%E6%9C%AF/"},{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"后台技术","slug":"后台技术","permalink":"http://yoursite.com/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Typescript","slug":"Typescript","permalink":"http://yoursite.com/tags/Typescript/"},{"name":"电影台词","slug":"电影台词","permalink":"http://yoursite.com/tags/%E7%94%B5%E5%BD%B1%E5%8F%B0%E8%AF%8D/"},{"name":"AutoEncoder","slug":"AutoEncoder","permalink":"http://yoursite.com/tags/AutoEncoder/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"},{"name":"连读发音","slug":"连读发音","permalink":"http://yoursite.com/tags/%E8%BF%9E%E8%AF%BB%E5%8F%91%E9%9F%B3/"},{"name":"GAN","slug":"GAN","permalink":"http://yoursite.com/tags/GAN/"},{"name":"协作开发软件","slug":"协作开发软件","permalink":"http://yoursite.com/tags/%E5%8D%8F%E4%BD%9C%E5%BC%80%E5%8F%91%E8%BD%AF%E4%BB%B6/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"},{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"Ted","slug":"Ted","permalink":"http://yoursite.com/tags/Ted/"},{"name":"Q网络","slug":"Q网络","permalink":"http://yoursite.com/tags/Q%E7%BD%91%E7%BB%9C/"},{"name":"GCN","slug":"GCN","permalink":"http://yoursite.com/tags/GCN/"},{"name":"沉默的大多数","slug":"沉默的大多数","permalink":"http://yoursite.com/tags/%E6%B2%89%E9%BB%98%E7%9A%84%E5%A4%A7%E5%A4%9A%E6%95%B0/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Flex","slug":"Flex","permalink":"http://yoursite.com/tags/Flex/"}]}